WEBVTT

1
00:00:00.000 --> 00:00:05.036
Hello and welcome back. 
This video, I want to talk about Gaussian 

2
00:00:05.036 --> 00:00:09.207
mixture models and its connection to 
sparse modeling. 

3
00:00:09.207 --> 00:00:15.267
And in particular, this is going to be 
useful for us to introduce the concept of 

4
00:00:15.267 --> 00:00:18.730
structured sparse models. 
Let's get into that. 

5
00:00:18.730 --> 00:00:24.900
As examples, we are going to use the same 
type of image processing tasks that we 

6
00:00:24.900 --> 00:00:30.270
have been using for regular sparse 
modeling in the previous videos. 

7
00:00:30.270 --> 00:00:36.040
What we have is that from an observation 
y, we want to recover the image f. 

8
00:00:36.040 --> 00:00:42.594
Now, the image f has been deteriorated by 
this operator U and has additive Gaussian 

9
00:00:42.594 --> 00:00:47.776
noise as we have seen before. 
What type of operators are we going to 

10
00:00:47.776 --> 00:00:53.644
consider, are we going to have in mind? 
The same type that we have seen before. 

11
00:00:53.644 --> 00:00:59.207
For example, U can be the masking. 
Basically, we let some pixels go through, 

12
00:00:59.207 --> 00:01:03.704
some pixels we block. 
This is why, basically, our observation 

13
00:01:03.704 --> 00:01:07.210
and we wanted to go back to f via 
inpainting. 

14
00:01:07.210 --> 00:01:12.111
U can be a convolution, so we're going to 
blur the image. 

15
00:01:12.111 --> 00:01:16.138
Once again, this is our y, 
this is the observation, 

16
00:01:16.138 --> 00:01:19.210
and we want to go back to f, do 
deblurring. 

17
00:01:19.210 --> 00:01:23.782
Or, we can do sub-sampling, 
basically, we want to go from a small 

18
00:01:23.782 --> 00:01:27.004
image to a larger image and that's 
zooming. 

19
00:01:27.004 --> 00:01:32.774
So, these are the type of operations that 
we are going to think and have in mind. 

20
00:01:32.774 --> 00:01:38.020
Now, let us describe the type of models 
that we are going to have in mind. 

21
00:01:38.020 --> 00:01:43.358
And the basic idea is that, for the 
moment, we are not thinking about sparse 

22
00:01:43.358 --> 00:01:46.916
modelling for f. 
We actually are going to be thinking 

23
00:01:46.916 --> 00:01:52.112
about Gaussian mixture models for f. 
Now, as we have been doing for sparse 

24
00:01:52.112 --> 00:01:55.315
modelling, 
we don't work with the whole image, 

25
00:01:55.315 --> 00:01:59.361
we work with patches. 
Once again, we work with all the 

26
00:01:59.361 --> 00:02:03.666
overlapping patches, 
all possible patches in the image. 

27
00:02:03.666 --> 00:02:07.015
And the idea is that we are going to mold 
it, 

28
00:02:07.015 --> 00:02:12.038
each one of these patches, f sub i. 
Again, that has been deformed. 

29
00:02:12.038 --> 00:02:17.220
Sometimes the operator is the same 
operator all across the image, 

30
00:02:17.220 --> 00:02:23.598
sometimes it's not, and that's why I mark 
it U sub i and this is our observation. 

31
00:02:23.598 --> 00:02:30.492
The noise is, as before, Gaussian. 
Now, we're going to model these signals 

32
00:02:30.492 --> 00:02:37.812
with K different Gaussians. 
So, basically we have, if the patch is 

33
00:02:37.812 --> 00:02:43.876
eight by eight. 
We are, we, again, have a 64-dimensional 

34
00:02:43.876 --> 00:02:51.471
vector as before, and we are going to 
model with K Gaussians, K 64-dimensional 

35
00:02:51.471 --> 00:02:55.797
Gaussians. 
For a Gaussian, we actually need to 

36
00:02:55.797 --> 00:03:00.699
compute the average. 
For each one of the K, just to have a 

37
00:03:00.699 --> 00:03:04.805
number in our head, we're going to make K 
= 10. 

38
00:03:04.805 --> 00:03:09.233
We are going to see that we only needed a 
few Gaussians. 

39
00:03:09.233 --> 00:03:14.869
So, we need to compute the average of 
this 64-dimensional vector and the 

40
00:03:14.869 --> 00:03:21.174
covariance of this 64-dimensional vector. 
Each one of the Gaussians, we need to do 

41
00:03:21.174 --> 00:03:25.003
that. 
Now, a Gaussian having a model as a 

42
00:03:25.003 --> 00:03:33.047
Gaussian is equivalent to having a model 
as a PCA or Principal Component Analysis. 

43
00:03:33.047 --> 00:03:40.797
The basic idea is that this covariance 
matrix, we can compute the item vectors 

44
00:03:40.797 --> 00:03:46.880
and the item values and the item vectors 
give us a dictionary. 

45
00:03:46.880 --> 00:03:52.765
64 atoms in that dictionary if we are in 
this dimension. 

46
00:03:52.765 --> 00:03:57.180
So each one of the Gaussians, let's say 
10, 

47
00:03:57.180 --> 00:04:03.159
gives us 64 atoms. 
We end up with a dictionary of 640 atoms 

48
00:04:03.159 --> 00:04:11.065
in this case, but in a very particular 
form, because each block of 64 atoms are 

49
00:04:11.065 --> 00:04:18.667
the eigenvectors that correspond to this, 
basically, this covariance matrix. 

50
00:04:18.667 --> 00:04:26.058
So that's our structure right now. And 
the basic idea is that each one of the 

51
00:04:26.058 --> 00:04:29.607
patches is modeled by one and only one of 
these. 

52
00:04:29.607 --> 00:04:35.422
And what we have to do is, from the noisy 
image, we are going to have to estimate 

53
00:04:35.422 --> 00:04:38.895
the noisy or the blur or the sub sample 
image. 

54
00:04:38.895 --> 00:04:44.181
If we are talking about in painting, we 
are going to have to estimate the 

55
00:04:44.181 --> 00:04:49.317
Gaussians, the 10 Gaussians, 
meaning, we are going to have to estimate 

56
00:04:49.317 --> 00:04:54.150
their average, we are going to have to 
estimate their covariance. 

57
00:04:54.150 --> 00:04:56.720
That's one thing that we have to 
estimate. 

58
00:04:56.720 --> 00:05:03.600
We also, are going to have to estimate 
for each patch, which one of these K or 

59
00:05:03.600 --> 00:05:10.660
10 Gaussians fits that patch the best. 
And then, once we do that, we're going to 

60
00:05:10.660 --> 00:05:16.468
have to obtain from the observation, 
reconstruct the actual image. 

61
00:05:16.468 --> 00:05:21.025
So, a lot of things that we are going to 
have to do. 

62
00:05:21.025 --> 00:05:26.714
Estimate the K Gaussians, 
estimate the identity of every patch, and 

63
00:05:26.714 --> 00:05:33.301
also then estimate the reconstruction. 
It turns out, although, this looks as a 

64
00:05:33.301 --> 00:05:37.016
very complicated problem, 
it turns out it is not. 

65
00:05:37.016 --> 00:05:43.285
It's a very simple problem and it can be 
efficiently solved with what's called a 

66
00:05:43.285 --> 00:05:46.845
maximum a posteriori expectation 
maximization. 

67
00:05:46.845 --> 00:05:50.483
The basic idea is that, if we know the 
Gaussian, 

68
00:05:50.483 --> 00:05:54.430
so, if we know every patch, 
the best Gaussian for it. 

69
00:05:54.430 --> 00:06:00.544
We have to do nothing else but linear 
operations with these type of filters to 

70
00:06:00.544 --> 00:06:05.252
do the reconstruction. 
So, it's piecewise linear for every 

71
00:06:05.252 --> 00:06:12.694
Gaussian, for every Gaussian that we use 
to feed the particular patch, we just 

72
00:06:12.694 --> 00:06:19.145
need a linear operator and, this is a 
very, very, simple algothorim. 

73
00:06:19.145 --> 00:06:26.190
Once again, we are approximating 
basically every one of the patches, but 

74
00:06:26.190 --> 00:06:32.202
by one Gaussian and it's kind of 
selecting from this very large 

75
00:06:32.202 --> 00:06:39.887
dictionary, one block of 64 atoms which 
corresponds to the Gaussian that we are 

76
00:06:39.887 --> 00:06:44.293
selecting. 
Now, the MAP-EM alternates between the 

77
00:06:44.293 --> 00:06:50.719
estimation of the Gaussian and estimation 
of the, basically, the appropriate, the 

78
00:06:50.719 --> 00:06:56.089
appropriate Gaussian for a given signal 
or for a given patch. 

79
00:06:56.089 --> 00:07:00.403
So we are doing, 
again, we are doing these maximum a 

80
00:07:00.403 --> 00:07:07.352
posteriori expectation maximization and 
that does more or less the following. 

81
00:07:07.352 --> 00:07:13.258
Estimates all of the Gaussians, 
then basically says, okay, which patches 

82
00:07:13.258 --> 00:07:17.603
like Gaussian number one more than any 
other Gaussian, 

83
00:07:17.603 --> 00:07:23.165
and then, with all those patches, 
reestimates the Gaussian number one. 

84
00:07:23.165 --> 00:07:28.540
And it's very similar to what we talk 
about the K-SVD, but instead of 

85
00:07:28.540 --> 00:07:34.322
estimating the dictionary atoms, we 
basically estimate a whole Gaussian, 

86
00:07:34.322 --> 00:07:39.009
meaning an average at covariance matrix 
or at PCA basis. 

87
00:07:39.009 --> 00:07:44.001
And then we keep iterating, 
normally, we do that for three, four 

88
00:07:44.001 --> 00:07:48.912
iterations until we converge. 
What we observe here is we have color 

89
00:07:48.912 --> 00:07:55.433
coded this image according to the 
Gaussian that a given pixel is selecting. 

90
00:07:55.433 --> 00:08:00.070
A pixel meaning the patch around it is 
selecting. 

91
00:08:00.070 --> 00:08:05.895
This is the initialization, or after one 
or two iterations, and this is after 

92
00:08:05.895 --> 00:08:09.344
that. 
And what we see is that similar regions 

93
00:08:09.344 --> 00:08:13.024
of the image end up selecting the same 
Gaussian. 

94
00:08:13.024 --> 00:08:19.002
Once again, we only allow, in this type 
of approach, to select a single Gaussian 

95
00:08:19.002 --> 00:08:24.291
for every patch, meaning for every pixel. 
And again, we are going to do the 

96
00:08:24.291 --> 00:08:30.040
recovery by averaging all the patches 
that touch the pixel as we have been 

97
00:08:30.040 --> 00:08:32.570
doing for sparse modeling. 
So now, 

98
00:08:32.570 --> 00:08:35.773
looks like we are not doing sparse 
modeling, 

99
00:08:35.773 --> 00:08:41.087
looks like what we are doing is 
representing every patch as one out of K 

100
00:08:41.087 --> 00:08:44.727
Gaussians. 
But let's see that what we are actually 

101
00:08:44.727 --> 00:08:49.140
doing is a very particular case of sparse 
modeling. 

102
00:08:49.140 --> 00:08:54.274
Why is that? 
In ordinary sparse modeling, we have a 

103
00:08:54.274 --> 00:08:58.382
large dictionary, 
normally, overcomplete, 

104
00:08:58.382 --> 00:09:04.851
and we are allow to select L atoms. 
So we have K here, a large K, 

105
00:09:04.851 --> 00:09:10.602
and we are allowed to select L atoms. 
In this case, three. 

106
00:09:10.602 --> 00:09:14.813
And remember, how many options do we 
have? 

107
00:09:14.813 --> 00:09:21.087
We have L to choose from K, and that is 
normally a very large number. 

108
00:09:21.087 --> 00:09:28.505
If we take the regular sizes that we are 
using for image processing by just which 

109
00:09:28.505 --> 00:09:36.828
are 5x5 or 8x8, and then, a dictionary 
which is 256 or 512 or 1000, and in 

110
00:09:36.828 --> 00:09:40.900
sparsity in the order of five to twelve 
five to ten, 

111
00:09:40.900 --> 00:09:46.074
this number is about this order. 
It's a huge number, a very large number 

112
00:09:46.074 --> 00:09:51.613
of subspaces that we can pick from that 
has the advantages, the advantage of 

113
00:09:51.613 --> 00:09:57.152
being a very rich model, but on the other 
hand, makes the model very unstable. 

114
00:09:57.152 --> 00:10:02.690
There is so much to choose from than 
basically if we make a small change in 

115
00:10:02.690 --> 00:10:08.011
the image or in the patch, we might end 
up selecting something completely 

116
00:10:08.011 --> 00:10:11.544
different. 
In, on the other hand, when we are 

117
00:10:11.544 --> 00:10:17.717
talking about the Gaussian mixture model, 
which as we say, is equivalent, 

118
00:10:17.717 --> 00:10:21.313
equivalent to a principal component 
analysis. 

119
00:10:21.313 --> 00:10:27.227
So we take the covariance matrix and we 
decompose it to get the principal 

120
00:10:27.227 --> 00:10:33.461
component analysis from the eigenvectors 
of the covariance matrix and then we 

121
00:10:33.461 --> 00:10:36.738
concatenate them. 
So this is one Gaussian, 

122
00:10:36.738 --> 00:10:41.613
another Gaussian, another Gaussian, 
another Gaussian, and so on. 

123
00:10:41.613 --> 00:10:44.650
And we have K Gaussians, 
let's say, 10. 

124
00:10:44.650 --> 00:10:51.443
Here are the eigenvectors corresponding 
to the covariance for the first Gaussian, 

125
00:10:51.443 --> 00:10:54.966
the second, the third, the fourth, and so 
on. 

126
00:10:54.966 --> 00:11:00.334
Here, I show that for five. 
Now, when you pick a Gaussian, you pick 

127
00:11:00.334 --> 00:11:06.855
the entire block. 
And then you'd fit the best you can that 

128
00:11:06.855 --> 00:11:12.365
Gaussian or you use the PCA basis as the 
atoms of your dictionary. 

129
00:11:12.365 --> 00:11:17.457
But look what happened. 
We went from this huge number to only 

130
00:11:17.457 --> 00:11:24.386
about 10 or 20 options, because once you 
pick the Gaussian, I mentioned it's in 

131
00:11:24.386 --> 00:11:29.686
your operation, you just project into it 
and you already have the atoms. 

132
00:11:29.686 --> 00:11:33.825
You already have everything. 
So in both cases, you learn the 

133
00:11:33.825 --> 00:11:37.262
dictionary. 
But here, you learn a dictionary where 

134
00:11:37.262 --> 00:11:43.135
every atom is independent and basically 
the coding with the atoms is independent. 

135
00:11:43.135 --> 00:11:47.266
Here we learn a dictionary that has a 
block structure. 

136
00:11:47.266 --> 00:11:53.004
And also when you are representing, when 
you are projecting, you pick entire 

137
00:11:53.004 --> 00:11:55.928
blocks. 
And this is a concept that is called 

138
00:11:55.928 --> 00:11:59.156
structure sparsity, 
where basically, the atoms have 

139
00:11:59.156 --> 00:12:04.192
correlations and they're either picked 
together or they are not picked at all. 

140
00:12:04.192 --> 00:12:08.323
This is a very particular case of what's 
called block structure. 

141
00:12:08.323 --> 00:12:13.729
There is no overlapping, because you are 
only allowed to pick one of the Gaussians 

142
00:12:13.729 --> 00:12:19.207
and there is no, basically in principle, 
there is no atom that belongs to more 

143
00:12:19.207 --> 00:12:23.262
than one of the Gaussians if the 
Gaussians are different. 

144
00:12:23.262 --> 00:12:28.670
So you only pick one block, which is a 
particular case of structure sparsity. 

145
00:12:28.670 --> 00:12:34.069
The overall concept of structure 
sparsity, being again, the fact that 

146
00:12:34.069 --> 00:12:39.310
atoms come and go together. 
They'll learn together, they are used to 

147
00:12:39.310 --> 00:12:45.503
encode the signals together, and that 
stabilizes your system much more because 

148
00:12:45.503 --> 00:12:51.418
the number of options is much lower. 
You either pick one and not the others or 

149
00:12:51.418 --> 00:12:55.290
you pick the other, 
but you're not allowed to pick another 

150
00:12:55.290 --> 00:12:58.628
from here, another from here, and another 
from here, 

151
00:12:58.628 --> 00:13:02.433
you pick in blocks. 
Now, of course, as before, there is a lot 

152
00:13:02.433 --> 00:13:07.240
of very interesting theory here, 
but it's also very important to know if 

153
00:13:07.240 --> 00:13:11.697
this is actually useful. 
If I need 10 million Gaussians, I'm not 

154
00:13:11.697 --> 00:13:15.696
gaining much. 
But actually, turns out that with 10 or 

155
00:13:15.696 --> 00:13:22.939
20 Gaussians, you can get basically some 
of the, some of the same results that we 

156
00:13:22.939 --> 00:13:28.372
saw for ordinary sparse coding, 
but with a much simpler algorithm and a 

157
00:13:28.372 --> 00:13:30.560
much more stable one. 
So it's, 

158
00:13:30.560 --> 00:13:36.394
again, it's sparse coding, sparse 
modeling, but in a structure fashion. 

159
00:13:36.394 --> 00:13:43.062
Let me show you a few examples of this. 
This is one of the examples that we have 

160
00:13:43.062 --> 00:13:47.277
seen before, 
but now with, with this Gaussian mixture 

161
00:13:47.277 --> 00:13:52.869
model or with this structure sparsity. 
Or basically, piecewise linear, 

162
00:13:52.869 --> 00:13:58.056
because once we picked the Gaussian, we 
are in the linear area. 

163
00:13:58.056 --> 00:14:02.991
Again, we have the original here, 
it's a zoom in to the original, 

164
00:14:02.991 --> 00:14:07.296
only 20% available, and here is the right 
construction. 

165
00:14:07.296 --> 00:14:11.072
We have seen a similar example with 
sparse coding. 

166
00:14:11.072 --> 00:14:15.000
Now, we are seeing it with the Gaussian 
mixture models. 

167
00:14:15.000 --> 00:14:21.992
Another example is zooming, we start from 
a very small image and we basically zoom 

168
00:14:21.992 --> 00:14:27.961
in, we want to make it larger and we get 
really, really nice results with this 

169
00:14:27.961 --> 00:14:33.418
technique, very sharp edges. 
So, this is another type of structure, 

170
00:14:33.418 --> 00:14:39.898
another type of sparsity with structure 
incorporated and also we get a very 

171
00:14:39.898 --> 00:14:46.550
strong connection with a classical model 
which is these Gaussian mixture models. 

172
00:14:46.550 --> 00:14:50.748
With this, we are actually concluding the 
week on sparse models. 

173
00:14:50.748 --> 00:14:55.947
We could spend a whole year teaching 
about sparse models, but we have provided 

174
00:14:55.947 --> 00:14:59.679
the key concepts. 
What this part coding, sparse coding, we 

175
00:14:59.679 --> 00:15:05.077
have provided the concept of how to learn 
the dictionaries, we have provided some 

176
00:15:05.077 --> 00:15:10.409
of the theory. We have talked about some 
of the theory. We have talked about some 

177
00:15:10.409 --> 00:15:15.541
of the computational techniques and we 
talked about the relationship between 

178
00:15:15.541 --> 00:15:20.540
sparse modeling and more classical 
techniques like Gaussian mixture models. 

179
00:15:20.540 --> 00:15:25.250
And from that, the new concept of 
structure sparsity. 

180
00:15:25.250 --> 00:15:31.282
This, as I say, is a topic which is very 
active research currently in the image 

181
00:15:31.282 --> 00:15:35.519
and signal processing community. 
And I hope you have enjoyed this week. 

182
00:15:35.519 --> 00:15:38.909
And I'm looking forward to see you next 
week once again. 

183
00:15:38.909 --> 00:15:40.120
Thank you very much. 