WEBVTT

1
00:00:00.000 --> 00:00:05.200
Hello, and welcome back. 
Before we continue with sparse modeling, 

2
00:00:05.200 --> 00:00:10.153
I want to make a side note on the topic 
of Compressed sensing. 

3
00:00:10.153 --> 00:00:16.344
Compressed sensing, basically, is using 
some of the same concepts that we have 

4
00:00:16.344 --> 00:00:21.165
been learning. 
But its goal is not to model a signal to 

5
00:00:21.165 --> 00:00:25.703
represent a signal, 
but the goal is to provide a new 

6
00:00:25.703 --> 00:00:30.590
something or a new sensing paradigm. 
Let us explain that. 

7
00:00:30.590 --> 00:00:37.168
We have, as we have before, a signal x, 
and we are going to consider that the 

8
00:00:37.168 --> 00:00:40.939
signal is exactly what we have seen 
before. 

9
00:00:40.939 --> 00:00:47.693
A dictionary multiply by a sparse vector. 
So, this is the signal that we want to 

10
00:00:47.693 --> 00:00:52.115
sense. 
In compressed sensing, we multiply that 

11
00:00:52.115 --> 00:00:56.196
by a matrix that reduces the number of 
rows. 

12
00:00:56.196 --> 00:01:02.967
So, we had a signal which was N 
dimensional and we are going to represent 

13
00:01:02.967 --> 00:01:10.295
it as dictionary times a sparse vector. 
But, we are not going to sense N points, 

14
00:01:10.295 --> 00:01:17.993
we're going to sense the product of these 
Q matrix with x, and Q is what's called a 

15
00:01:17.993 --> 00:01:21.240
fat matrix. 
So, it has, here, it has N. 

16
00:01:21.240 --> 00:01:27.175
But here, it has less than N. 
So, we are in a sense less that's why 

17
00:01:27.175 --> 00:01:33.290
it's called compressed sensing. 
Now, let us write that down in formulas. 

18
00:01:33.290 --> 00:01:40.890
We have D alpha equal x, that's what we 
have here. 

19
00:01:40.890 --> 00:01:46.777
But we multiply both sides by Q, the 
sensing matrix. 

20
00:01:46.777 --> 00:01:54.697
So, Q times D defines a new dictionary, 
this D tilde, is kind of a new 

21
00:01:54.697 --> 00:02:03.153
dictionary, times alpha, the same sparse 
vector is equal not to x but to x tilde, 

22
00:02:03.153 --> 00:02:08.826
the sense vector which is shorter than it 
was before. 

23
00:02:08.826 --> 00:02:13.960
So, we end up basically sensing less 
points. 

24
00:02:13.960 --> 00:02:19.523
And the goal is to recover from this, 
which is what was sensed, and the 

25
00:02:19.523 --> 00:02:25.324
knowledge of D, because we know the 
sensing matrix, we know the dictionary. 

26
00:02:25.324 --> 00:02:30.331
The goal is to recover alpha. 
So mathematically, this looks very 

27
00:02:30.331 --> 00:02:35.497
similar to sparse modeling. 
We have a signal, we have a dictionary 

28
00:02:35.497 --> 00:02:41.065
and we need to reconstruct alpha. 
Now, what compressed sensing does, it 

29
00:02:41.065 --> 00:02:47.390
deals with this fundamental problem. 
And basically, the idea is to provide 

30
00:02:47.390 --> 00:02:51.116
conditions for the recovery to be 
possible. 

31
00:02:51.116 --> 00:02:58.047
The conditions have to be on Q, the 
sensing matrix, they have to be on D, the 

32
00:02:58.047 --> 00:03:02.639
dictionary that we use for the sparse 
representation. 

33
00:03:02.639 --> 00:03:06.364
And also, on the level of sparsity of 
alpha. 

34
00:03:06.364 --> 00:03:11.650
So, compressed sensing provides very nice 
mathematical theory. 

35
00:03:11.650 --> 00:03:15.822
And conditions under which we can do the 
recovery. 

36
00:03:15.822 --> 00:03:22.936
If we can recover alpha from the sense x 
tilde, then we will be able to recover 

37
00:03:22.936 --> 00:03:29.087
the original signal x by doing D alpha. 
And, once again, the same way that we 

38
00:03:29.087 --> 00:03:35.648
have a lot of interesting math in sparse 
modeling, there's a lot of related math 

39
00:03:35.648 --> 00:03:41.225
in compressed sensing that provides 
conditions for this to be possible. 

40
00:03:41.225 --> 00:03:45.490
But, it's very important to make a clear 
distinction. 

41
00:03:45.490 --> 00:03:51.340
What we have been discussing about sparse 
modeling is the model of signals. 

42
00:03:51.340 --> 00:03:57.170
Compressed sensing is about designing new 
sensing protocols. 

43
00:03:57.170 --> 00:04:01.850
Now, for what type of signals with a sign 
new sense in protocols? 

44
00:04:01.850 --> 00:04:07.647
Most of compressed sensing has been 
dealing with the design of new sensing 

45
00:04:07.647 --> 00:04:13.521
protocols for sparse signals. Meaning, 
signals that can be represented with a 

46
00:04:13.521 --> 00:04:19.010
dictionary in a sparse fashion. 
So, those use the same type of tools but 

47
00:04:19.010 --> 00:04:24.498
they're very different goals. 
And basically, what compressed sensing is 

48
00:04:24.498 --> 00:04:30.836
trying to do is trying to extend Nyquist, 
which is a classical sampling or sensing 

49
00:04:30.836 --> 00:04:36.657
theory for band limited signals. 
Compressed sensing says, what should be 

50
00:04:36.657 --> 00:04:40.948
the sampling or sensing theory for sparse 
signals? 

51
00:04:40.948 --> 00:04:47.300
And that's the relationship between 
compressed sensing and sparse modeling. 

52
00:04:47.300 --> 00:04:50.780
After this very short note on compressed 
sensing, 

53
00:04:50.780 --> 00:04:56.706
and we could spend weeks teaching about 
compressed sensing but it's not the topic 

54
00:04:56.706 --> 00:05:01.890
of this week or of this, of this class. 
But, after this short note, we're going 

55
00:05:01.890 --> 00:05:07.667
to come back in the next video with one 
more concept in the area of sparse 

56
00:05:07.667 --> 00:05:09.296
modeling. 
See you then. 

57
00:05:09.296 --> 00:05:10.778
Thank you very much. 