WEBVTT

1
00:00:00.000 --> 00:00:04.880
Hello and welcome back. 
We're starting a new week in the area of 

2
00:00:04.880 --> 00:00:09.455
image and video processing. 
The topic of this week is Sparse 

3
00:00:09.455 --> 00:00:13.192
Modeling. 
This is one of the most active areas in 

4
00:00:13.192 --> 00:00:18.377
image processing currently. 
And is very important for us to describe 

5
00:00:18.377 --> 00:00:21.846
this area. 
We're going to be using some basic tools 

6
00:00:21.846 --> 00:00:25.740
of linear algebra. 
So there's going to be a bit of math, but 

7
00:00:25.740 --> 00:00:30.690
not as much as we had for example during 
the week on PDE's, and variational 

8
00:00:30.690 --> 00:00:35.641
calculus and differential geometry. 
And as before, everything is going to be 

9
00:00:35.641 --> 00:00:38.875
self-contained and I'm going to walk you 
through. 

10
00:00:38.875 --> 00:00:43.760
So it's going to be very, very simple, 
and you're going to be able to catch. 

11
00:00:43.760 --> 00:00:49.366
Everything that you need to know. 
The fundamentals in Sparse Modeling to 

12
00:00:49.366 --> 00:00:55.200
understand, as I say, one of the most 
active current areas in the area of image 

13
00:00:55.200 --> 00:00:59.687
and video processing. 
Before I proceed, I should mention that 

14
00:00:59.687 --> 00:01:05.446
I'm going to be using slides that I have 
adapted with permission from Professor 

15
00:01:05.446 --> 00:01:08.785
Ilad. 
I want to let you know also that if you 

16
00:01:08.785 --> 00:01:14.095
want to learn more about this topic, his 
book is an excellent source for that. 

17
00:01:14.095 --> 00:01:19.198
Of course, you don't need the book. 
Everything is going to be set, contained. 

18
00:01:19.198 --> 00:01:24.646
You don't need to buy the book to enjoy 
what we're going to be learning during 

19
00:01:24.646 --> 00:01:29.197
this week but if you want to pursue 
further this area, it's a good 

20
00:01:29.197 --> 00:01:34.438
recommended source of material and 
there's also some other presentations on 

21
00:01:34.438 --> 00:01:37.811
the web. 
And also in the class web page. 

22
00:01:37.811 --> 00:01:45.093
In this class web page, we have listed a 
couple of places where you can get free 

23
00:01:45.093 --> 00:01:50.555
software to play with this area. 
So, let's proceed to percent. 

24
00:01:50.555 --> 00:01:55.668
What is Sparse Modeling? 
And we are going to use as example image 

25
00:01:55.668 --> 00:01:59.368
denoising. 
Sparse Modeling is used for many, many 

26
00:01:59.368 --> 00:02:04.995
other areas in image processing, but 
image denoising is a good example to 

27
00:02:04.995 --> 00:02:10.313
introduce the basic concept. 
So as we have seen before, the basic idea 

28
00:02:10.313 --> 00:02:13.320
is that we're going to have a noise 
image. 

29
00:02:13.320 --> 00:02:19.452
Which we assume again just because we 
want a very simple presentation of the 

30
00:02:19.452 --> 00:02:24.231
topic that we are having additive noise 
added to this image. 

31
00:02:24.231 --> 00:02:30.284
So, its the image plus noise and we 
want to use Sparse Modeling to remove the 

32
00:02:30.284 --> 00:02:33.470
noise. 
Or from a noisy image we want to do 

33
00:02:33.470 --> 00:02:38.367
something to remove the noise. 
Now this can be formulated in the 

34
00:02:38.367 --> 00:02:43.604
following form, which is type of a 
variational formulation that we have 

35
00:02:43.604 --> 00:02:47.882
discussed before. 
But now we are in the discreet domain, so 

36
00:02:47.882 --> 00:02:52.160
it's kind of a bit simpler. 
We have a couple of components. 

37
00:02:52.160 --> 00:02:58.048
Why is our image the noisy image that's 
all we have. 

38
00:02:58.048 --> 00:03:04.660
We measure a noisy image. 
And we want to recover a clean image. 

39
00:03:04.660 --> 00:03:08.111
That's X. 
Now, the first thing is, we don't want 

40
00:03:08.111 --> 00:03:12.462
the recover image to be too far away from 
the noise image. 

41
00:03:12.462 --> 00:03:15.914
And that's the penalization that we have 
here. 

42
00:03:15.914 --> 00:03:21.916
Is that means square error, between the 
image that we observe and the image that 

43
00:03:21.916 --> 00:03:25.442
we recover. 
We have seen the means square error. 

44
00:03:25.442 --> 00:03:31.220
For those that want understandably more, 
what we're assuming here is additive. 

45
00:03:31.220 --> 00:03:35.560
Gaussian noise. 
And basically we are minimizing and this 

46
00:03:35.560 --> 00:03:41.684
is the variance of the noise, basically. 
Now if we only had this term which fits 

47
00:03:41.684 --> 00:03:46.180
the measurements, then what's the 
solution to this problem? 

48
00:03:46.180 --> 00:03:49.668
What's the image that minimizes this 
problem? 

49
00:03:49.668 --> 00:03:54.630
Its nothing than the noisy image itself. 
So, we haven't done much. 

50
00:03:54.630 --> 00:04:00.711
And that's when we add another term that 
has multiple names depending on the 

51
00:04:00.711 --> 00:04:02.606
discipline. 
It's a prior. 

52
00:04:02.606 --> 00:04:08.530
It's also called a regularization term 
that basically says, yes, I know what 

53
00:04:08.530 --> 00:04:14.690
image, I know I want an image that is 
close to y, but I want that image to have 

54
00:04:14.690 --> 00:04:19.113
certain properties. 
Let me say this straight one example. 

55
00:04:19.113 --> 00:04:24.800
Let's assume that this is your data. 
I'm going to just mark a few points. 

56
00:04:27.760 --> 00:04:33.498
This is your data, this is Y. 
If I tell you nothing else, but find X 

57
00:04:33.498 --> 00:04:37.584
that is close to Y, you're going to stay 
with that. 

58
00:04:37.584 --> 00:04:42.589
But if I tell you. 
That through this function, if I tell you 

59
00:04:42.589 --> 00:04:48.714
that the solution is a straight line, I 
force you to make a straight line. 

60
00:04:48.714 --> 00:04:54.000
Then, you're going to find a solution 
which is something like that. 

61
00:04:54.000 --> 00:04:57.513
Not the original points but something 
like that. 

62
00:04:57.513 --> 00:05:02.637
So I gave you prior information. 
I gave you a constrain that says, find 

63
00:05:02.637 --> 00:05:08.053
something that is close but is also a 
straight line and then you got that 

64
00:05:08.053 --> 00:05:11.494
solution. 
So the basic idea is that we have two 

65
00:05:11.494 --> 00:05:14.568
terms. 
One that relates dissolution to the 

66
00:05:14.568 --> 00:05:18.798
measurements. 
And one that basically gives you a prior, 

67
00:05:18.798 --> 00:05:24.156
a regularization at condition. 
So it's kind of we're projecting the 

68
00:05:24.156 --> 00:05:29.514
observation, Y, into this prior. 
Now, this is what's called a Bayesian 

69
00:05:29.514 --> 00:05:35.112
point of view, following Thomas Bayes, 
and the basic idea is that we're 

70
00:05:35.112 --> 00:05:39.830
computing what's called the 
Maximum-A-posteriori or the map. 

71
00:05:39.830 --> 00:05:44.811
We're basically going to compute. 
The X that maximizes something, or 

72
00:05:44.811 --> 00:05:49.441
minimizes this function. 
Depending if we take the function as 

73
00:05:49.441 --> 00:05:54.907
itself, we call it a minimizer. 
If we look at the probabilistic framework 

74
00:05:54.907 --> 00:06:01.132
that this, basically an exponentiation, 
so we take this to the exponent and this, 

75
00:06:01.132 --> 00:06:05.535
also kind of, to the exponent, that would 
be a maximization. 

76
00:06:05.535 --> 00:06:09.787
But don't worry. 
We do max or we do min, depending on the 

77
00:06:09.787 --> 00:06:12.520
sign and depending. 
We can always do. 

78
00:06:12.520 --> 00:06:17.233
Minus and then we get a maximization, so 
that's why it's called the 

79
00:06:17.233 --> 00:06:21.454
Maximum-A-posteriori. 
We could give to everything that we are 

80
00:06:21.454 --> 00:06:26.379
going to be presenting next, a 
probabilistic framework, a probabilistic 

81
00:06:26.379 --> 00:06:29.897
interpretation and then make this even 
more clear. 

82
00:06:29.897 --> 00:06:34.962
But the basic idea is that we have a 
prior, and in base language this is 

83
00:06:34.962 --> 00:06:40.262
called a likelihood. 
The prior and the likelihood and both can 

84
00:06:40.262 --> 00:06:46.824
have a probabilistic interpretation. 
Now, Once again, this models the noise, 

85
00:06:46.824 --> 00:06:51.495
In this case, additive Gaussian. 
This models the signal. 

86
00:06:51.495 --> 00:06:58.589
And a lot of image and in general signal 
processing has dedicated a lot of effort 

87
00:06:58.589 --> 00:07:05.855
into designing what is the best prior? 
What is the best space to define images? 

88
00:07:05.855 --> 00:07:12.516
To define different types of signals. 
And there has been a lot of work in the 

89
00:07:12.516 --> 00:07:16.150
literature of image processing in that. 
So. 

90
00:07:16.150 --> 00:07:21.492
I'm going to just describe a few. 
People say, a good prior is, give me an 

91
00:07:21.492 --> 00:07:26.309
image that doesn't have too much energy. 
So this is this example. 

92
00:07:26.309 --> 00:07:31.050
There is always a parameter here that can 
scale it up and down. 

93
00:07:31.050 --> 00:07:35.478
There were other priors that say: give me 
an image that is smooth. 

94
00:07:35.478 --> 00:07:40.725
For example, we have seen when we talk 
about in painting, that one way to make 

95
00:07:40.725 --> 00:07:45.767
sure of smoothness is roughness. 
Instead of having low energy, you say, it 

96
00:07:45.767 --> 00:07:49.719
has to be very smooth. 
Which means that, when I compute the 

97
00:07:49.719 --> 00:07:55.748
roughness, the energy has to be low. 
Another prior is to say, that's correct 

98
00:07:55.748 --> 00:08:00.785
but we have edges. 
So I don't want my edges to be smooth. 

99
00:08:00.785 --> 00:08:06.452
I want very sharp edges. 
And then we do an adaptation that says 

100
00:08:06.452 --> 00:08:11.760
not every place it has to be smooth. 
We allow certain jumps. 

101
00:08:11.760 --> 00:08:16.682
Another example, is to basically take 
functions, not just the quadratic 

102
00:08:16.682 --> 00:08:21.393
function, but more sophisticated 
functions of smoothness and that's 

103
00:08:21.393 --> 00:08:24.346
related to the topic of robust 
statistics. 

104
00:08:24.346 --> 00:08:29.831
Just another example, we can talk about 
total variation, we have seen that when 

105
00:08:29.831 --> 00:08:34.964
we need initial tropic diffusion. 
This was one of the examples of initial 

106
00:08:34.964 --> 00:08:40.238
tropic diffusion that we took basically 
the integral over the gradient, not 

107
00:08:40.238 --> 00:08:45.090
gradient square, but the gradient and we 
got initial tropic diffusion. 

108
00:08:45.090 --> 00:08:49.877
We can also do wavelengths, and we 
haven't discussed in this class about 

109
00:08:49.877 --> 00:08:53.733
wavelengths, but you probably have heard 
about wavelengths. 

110
00:08:53.733 --> 00:08:56.924
It's a very important topic in image 
processing. 

111
00:08:56.924 --> 00:09:01.446
Of course, in nine weeks, we cannot 
discuss everything, so we have not 

112
00:09:01.446 --> 00:09:04.152
discussed about. 
Wavelets in particular. 

113
00:09:04.152 --> 00:09:08.438
But people have used that. 
And you can think about basically 

114
00:09:08.438 --> 00:09:13.938
multiplying the matrix, multiplying the 
image by a matrix, and then doing some 

115
00:09:13.938 --> 00:09:18.438
penalty on that product. 
For example, the L1, as we have here, the 

116
00:09:18.438 --> 00:09:22.460
absolute value. 
What we're going to be discussing during 

117
00:09:22.460 --> 00:09:26.273
this week is this particular prior that 
we have here. 

118
00:09:26.273 --> 00:09:32.101
And I am going to explain it more in the 
next few slides and we're even going to 

119
00:09:32.101 --> 00:09:37.282
learn more in the next videos. 
But this it the prior that we're going to 

120
00:09:37.282 --> 00:09:40.807
be talking about. 
And basically, it's written here. 

121
00:09:40.807 --> 00:09:46.132
But again, I'm going to explain it next. 
This has been basically the evolution 

122
00:09:46.132 --> 00:09:48.528
and. 
Basically, is kind of historical 

123
00:09:48.528 --> 00:09:52.680
evolution, they'll know exactly about 
different types of priors. 

124
00:09:52.680 --> 00:09:56.420
That people have used, that people have 
proposed. 

125
00:09:56.420 --> 00:09:59.727
Each one has it's advantages and 
disadvantages. 

126
00:09:59.727 --> 00:10:05.052
We are going to discuss this one. 
Now there's much more sophisticted priors 

127
00:10:05.052 --> 00:10:08.197
that people have proposed in the 
literature. 

128
00:10:08.197 --> 00:10:13.629
Some of those are very good but are 
computationally extremely expensive and 

129
00:10:13.629 --> 00:10:17.632
extremely difficult. 
One of the features of many of these 

130
00:10:17.632 --> 00:10:23.064
priors and as we're going to see also of 
this one its computations are visible. 

131
00:10:23.064 --> 00:10:27.139
We can do them. 
We can understand the prior and we can do 

132
00:10:27.139 --> 00:10:30.570
the computations. 
So let us explain what is this. 

133
00:10:30.570 --> 00:10:34.260
Prior of Sparse Modeling. 