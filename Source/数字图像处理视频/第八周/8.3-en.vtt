WEBVTT

1
00:00:00.000 --> 00:00:04.402
Hello and welcome back. 
During this video I want to talk about 

2
00:00:04.402 --> 00:00:10.345
the implementation of sparse modeling, in 
other words how do we compute the alpha 

3
00:00:10.345 --> 00:00:15.921
that we have seen in the previous videos? 
During the video we are also going to 

4
00:00:15.921 --> 00:00:21.498
discuss some theoretical results, I'm 
just going to mention their existence and 

5
00:00:21.498 --> 00:00:25.480
why they are so important. 
Let's start with that. 

6
00:00:25.480 --> 00:00:32.512
Assume that we have just produced a 
signal X as this product, basically as a 

7
00:00:32.512 --> 00:00:37.417
sparse model. 
So assume that we know the dictionary D 

8
00:00:37.417 --> 00:00:44.912
and we randomly pick an alpha with a few 
non zero coefficients and we generate X. 

9
00:00:44.912 --> 00:00:51.760
And now our goal is to find back this 
alpha, so now we know the dictionary. 

10
00:00:51.760 --> 00:00:58.523
And, we know the signal and we want to 
compute it's sparse representation. 

11
00:00:58.523 --> 00:01:04.228
That's our goal right now. 
As we said later on we are going to 

12
00:01:04.228 --> 00:01:09.070
discuss about how to compute the 
dictionary itself, so. 

13
00:01:09.070 --> 00:01:15.707
As we have seen in the previous video, we 
want to compute the alpha that has the 

14
00:01:15.707 --> 00:01:21.763
smallest possible number of non-serial 
coefficients, that's also called a 

15
00:01:21.763 --> 00:01:26.326
support, and we assume that there is no 
noise right now. 

16
00:01:26.326 --> 00:01:32.134
We have just generated this signal, so 
it's kind of the ideal scenario. 

17
00:01:32.134 --> 00:01:37.339
So that's our goal right now. 
And there are a number of questions. 

18
00:01:37.339 --> 00:01:43.636
One of the questions is why would we get, 
with whatever algorithm that we produce 

19
00:01:43.636 --> 00:01:47.835
here, that we are going to explain in the 
next few slides. 

20
00:01:47.835 --> 00:01:53.511
Why are we expected to get exactly the 
same alpha that produce the sigma? 

21
00:01:53.511 --> 00:01:59.746
In other words, is that alpha unique? 
Can you think about a scenario where we 

22
00:01:59.746 --> 00:02:03.955
know that alpha non necessarily has to be 
unique? 

23
00:02:03.955 --> 00:02:10.140
And can you think of a scenario where we 
know that alpha will be unique? 

24
00:02:10.140 --> 00:02:14.420
Let's think for a second about that 
problem. 

25
00:02:14.420 --> 00:02:22.844
One example where alpha is not unique if 
for example several of the columns of D 

26
00:02:22.844 --> 00:02:25.390
are identical. 
So. 

27
00:02:25.390 --> 00:02:30.660
Let's assume that this was generated by 
one of those columns. 

28
00:02:30.660 --> 00:02:36.079
I can actually pick any other column, 
because they're identical, and I can 

29
00:02:36.079 --> 00:02:40.830
replace that alpha by, by the alpha 
corresponding to that column. 

30
00:02:40.830 --> 00:02:44.245
I assume that column one or two are 
identical. 

31
00:02:44.245 --> 00:02:49.738
I assume that, basically, I generated 
this signal with exactly this pattern. 

32
00:02:49.738 --> 00:02:52.782
So, I generated it with the second 
column. 

33
00:02:52.782 --> 00:02:58.498
But when I go to reconstruct, I can pick 
column one, you still have column two, 

34
00:02:58.498 --> 00:03:00.800
and therefore, I get that different 
alpha. 

35
00:03:00.800 --> 00:03:05.420
So that's an example where basically, the 
solution is not unique. 

36
00:03:05.420 --> 00:03:11.270
An example where the solution is unique 
is in DCT, Discrete Cosine Transform or 

37
00:03:11.270 --> 00:03:16.157
in the Fourier Transform. 
We know that actually the transform of a 

38
00:03:16.157 --> 00:03:20.675
Signalling Fourier or in Discrete Cosine 
Transform is unique. 

39
00:03:20.675 --> 00:03:24.230
So those are cases, now where was the 
difference? 

40
00:03:24.230 --> 00:03:28.584
In one of the scenarios, D was basically 
had the repetitions. 

41
00:03:28.584 --> 00:03:33.955
In the other scenario we know that 
Fourier and the cosine every column is 

42
00:03:33.955 --> 00:03:38.382
independent of the rest, they are both 
orthogonal to the rest. 

43
00:03:38.382 --> 00:03:43.898
So it looks like the structure of the 
dictionary D has a lot to do with the 

44
00:03:43.898 --> 00:03:48.110
uniqueness of alpha. 
Here comes the second question. 

45
00:03:48.110 --> 00:03:52.812
Will I get, because in the previous case 
at least the support, the number of 

46
00:03:52.812 --> 00:03:56.950
non-zero proficients, meaning the size of 
the support, was the same. 

47
00:03:56.950 --> 00:04:03.526
Could I get basically maybe even a more 
sparse representation than the one that 

48
00:04:03.526 --> 00:04:07.555
produce it? 
And, once again it's not hard to think 

49
00:04:07.555 --> 00:04:12.981
about of an example of that. 
Assume that basically you generated a 

50
00:04:12.981 --> 00:04:16.434
signal with all these three, and this is 
X. 

51
00:04:16.434 --> 00:04:21.531
Now assume that by chance that X was part 
of the dictionary D. 

52
00:04:21.531 --> 00:04:27.203
Then when you're going to try to recover, 
your sparsity goes back to 1. 

53
00:04:27.203 --> 00:04:32.535
So you went from three to 1. 
What's, what's the problem there once 

54
00:04:32.535 --> 00:04:35.632
again? 
One of the columns is basically a 

55
00:04:35.632 --> 00:04:40.820
combination of the others, so if you 
produce a signal that has such 

56
00:04:40.820 --> 00:04:45.128
combination. 
And then you might be able to get it back 

57
00:04:45.128 --> 00:04:50.759
without actually less coefficients. 
So these two problems, actually the 

58
00:04:50.759 --> 00:04:54.380
questions depend a lot on a number of 
things. 

59
00:04:54.380 --> 00:04:59.817
The dictionary, meaning it's size k, and 
also its general structure. 

60
00:04:59.817 --> 00:05:05.832
And the basic idea, as we have seen from 
this example, is that we want the 

61
00:05:05.832 --> 00:05:12.176
dictionary D to have atoms, columns that 
are as anchor related as possible to 

62
00:05:12.176 --> 00:05:16.296
guarantee that basically we have unique 
solutions. 

63
00:05:16.296 --> 00:05:22.393
And that basically we get back the 
sparsest signal that actually produced 

64
00:05:22.393 --> 00:05:25.718
the signal. 
So depending on the sparsity level. 

65
00:05:25.718 --> 00:05:29.795
Basically, depending on the number of 
non-zero coefficients. 

66
00:05:29.795 --> 00:05:35.392
And on some conditions on the dictionary. 
There is beautiful theoretical research 

67
00:05:35.392 --> 00:05:40.298
out there that guarantee uniqueness. 
And that guarantee that we get the, 

68
00:05:40.298 --> 00:05:45.412
basically, smallest possible support when 
we do the type of reconstruction 

69
00:05:45.412 --> 00:05:50.717
algorithms that I'm going to show next. 
So it's important for us to know the 

70
00:05:50.717 --> 00:05:56.429
existence of this very important 
theoretical resides in the background 

71
00:05:56.429 --> 00:06:02.522
that support the type of techniques that 
we are developing and that support the 

72
00:06:02.522 --> 00:06:05.950
use and the exploitation of sparse 
modelling. 

73
00:06:05.950 --> 00:06:12.476
Our goal now, assuming the theoretical 
results, is actually to solve sparse 

74
00:06:12.476 --> 00:06:14.545
modeling. 
So we want to find. 

75
00:06:14.545 --> 00:06:19.480
Now, we are giving a signal, y. 
Remem-, now is not the ideal case 

76
00:06:19.480 --> 00:06:22.823
anymore. 
X is going to be represented by this. 

77
00:06:22.823 --> 00:06:27.200
So this is our x. 
We want to find their representation of 

78
00:06:27.200 --> 00:06:30.852
x. 
Given the dictionary d such that we get 

79
00:06:30.852 --> 00:06:37.837
the smallest possible super meaning the 
smallest possible non zero coefficients 

80
00:06:37.837 --> 00:06:42.290
and we don't get too far away from the 
observation. 

81
00:06:42.290 --> 00:06:46.413
And that's our goal right now. 
To solve this problem. 

82
00:06:46.413 --> 00:06:52.441
This is actually what's called a 
commutatorial problem, and it's MP. 

83
00:06:52.441 --> 00:06:58.468
It's actually, you cannot solve this 
problem as I just posed it in basically 

84
00:06:58.468 --> 00:07:01.720
in realistic time. 
Let's just explain why. 

85
00:07:01.720 --> 00:07:07.378
We're going to basically do the 
following, which is the only way to solve 

86
00:07:07.378 --> 00:07:12.390
exactly this problem. 
We start with L equal one, so we look for 

87
00:07:12.390 --> 00:07:17.644
a signal that has sparcity one. 
That's what we are going to do first. 

88
00:07:17.644 --> 00:07:24.030
So, we generate all the possible supports 
of that signal, because L is equal one, 

89
00:07:24.030 --> 00:07:30.173
basically the support means only the 
first one is active, or only the second 

90
00:07:30.173 --> 00:07:35.120
one is active, or only the third one is 
active, and so on. 

91
00:07:35.120 --> 00:07:40.088
Now, how many possibilities we have? 
We have already seen that. 

92
00:07:40.088 --> 00:07:45.056
We have l chosen out of k. 
In the case of l equal one we have 

93
00:07:45.056 --> 00:07:49.699
basically k possibilities. 
When we are going to iterate this, 

94
00:07:49.699 --> 00:07:55.726
because then we are going to do k equal 
two, we have of the order of k square 

95
00:07:55.726 --> 00:07:59.798
possibilities. 
And as l increases, we're going to have 

96
00:07:59.798 --> 00:08:05.173
more and more possibilities. 
But for now, we basically have all the 

97
00:08:05.173 --> 00:08:07.930
possible supports. 
Now with. 

98
00:08:07.930 --> 00:08:14.094
Each one of those possible supports will 
basically go and solve the fitting 

99
00:08:14.094 --> 00:08:17.907
problem. 
So we say okay, what happen if I'm only 

100
00:08:17.907 --> 00:08:22.936
going to use the first atom? 
All what's left is to pick the scale. 

101
00:08:22.936 --> 00:08:28.533
How much of that first atom will I use? 
And then I solve this problem. 

102
00:08:28.533 --> 00:08:34.130
This is a least squares problem. 
It's a very simple quadratic problem. 

103
00:08:34.130 --> 00:08:39.624
Many ways to solve it, but the basic idea 
is that we are going to project the 

104
00:08:39.624 --> 00:08:43.187
signal into the atom that we have just 
selected. 

105
00:08:43.187 --> 00:08:46.900
And we see basically we try for each one 
of those. 

106
00:08:46.900 --> 00:08:53.163
And we actually seen what's the error. 
If the error is less than what we want 

107
00:08:53.163 --> 00:08:58.372
then we're done. 
We basically peak the alpha that produced 

108
00:08:58.372 --> 00:09:04.020
that small error. 
Now if there is not then we increase the 

109
00:09:04.020 --> 00:09:08.744
support. 
So, we have just tried, basically, with L 

110
00:09:08.744 --> 00:09:12.865
equal one. 
Now let's try with L equal two. 

111
00:09:12.865 --> 00:09:21.207
So we pick all the possible supports that 
have two none two active, two non-zero 

112
00:09:21.207 --> 00:09:24.022
coefficients. 
So we say okay, 

113
00:09:24.022 --> 00:09:31.442
how can I approximate the signal Y with 
the first and second atom together, or 

114
00:09:31.442 --> 00:09:37.966
with the first and third atom together, 
but with the seventh and the twenty fifth 

115
00:09:37.966 --> 00:09:41.590
atom together. 
So we pick every possible tool. 

116
00:09:41.590 --> 00:09:48.541
Again there are two chosen out of K which 
is in the order of K^2 and we try all of 

117
00:09:48.541 --> 00:09:54.609
them we solve for each one of them this 
problem again just a least squares 

118
00:09:54.609 --> 00:10:00.919
problem or a projection on to the 
sub-space generated by the selected atoms 

119
00:10:00.919 --> 00:10:06.340
we try again we keep going and we keep 
going so, thus we solve this. 

120
00:10:06.340 --> 00:10:11.956
Basically absolutely no problem. 
What's the problem with this technique. 

121
00:10:11.956 --> 00:10:15.832
The problem with this technique is the 
following. 

122
00:10:15.832 --> 00:10:18.759
Assume that K1000. 
= 1000, that's a standard size. 

123
00:10:18.759 --> 00:10:25.088
It's a good size as I say it's about the 
size that we're going to be using when we 

124
00:10:25.088 --> 00:10:30.388
talk about imagery noising. 
And assume that I tell you they have to 

125
00:10:30.388 --> 00:10:34.660
be ten atoms. 
So basically it's ten chosen out of 1000. 

126
00:10:34.660 --> 00:10:41.078
That's about of the order, starts being 
the order of possibilities starts, starts 

127
00:10:41.078 --> 00:10:46.132
being about the order of 1000 to the 
tenth something like that. 

128
00:10:46.132 --> 00:10:52.091
So, it's a very, very large number. 
Assume that each one of the computations 

129
00:10:52.091 --> 00:10:55.807
that we have here takes about one nano 
second. 

130
00:10:55.807 --> 00:10:59.442
So, you try all possible supports of size 
ten. 

131
00:10:59.442 --> 00:11:02.834
You solve this problem in one nano 
second. 

132
00:11:02.834 --> 00:11:08.595
These are all reasonable numbers. 
What you get is that you're going to need 

133
00:11:08.595 --> 00:11:13.277
a long, long time to solve this problem. 
Look at this number. 

134
00:11:13.277 --> 00:11:17.721
It's a huge number. 
That's how much you're going to need to 

135
00:11:17.721 --> 00:11:22.482
solve the problem if I already gave you 
that L is equal ten. 

136
00:11:22.482 --> 00:11:27.800
Imagine what would happen if you have to 
run over all possible L's. 

137
00:11:27.800 --> 00:11:33.507
It's impossible. 
That's why the problem is very hard. 

138
00:11:33.507 --> 00:11:40.290
It's impossible to solve as it is. 
So, are we done with, wick, with this 

139
00:11:40.290 --> 00:11:44.212
week? 
We just presented a great model that we 

140
00:11:44.212 --> 00:11:48.483
cannot solve for it. 
No of course we are not done. 

141
00:11:48.483 --> 00:11:52.841
Now we are going to have to find a way to 
solve this. 

142
00:11:52.841 --> 00:11:58.520
How do we do that? 
There is actually a couple of fundamental 

143
00:11:58.520 --> 00:12:04.980
ways to solve this problem. 
Again, a good fitting, with as sparse as 

144
00:12:04.980 --> 00:12:11.322
possible representation. 
There are again two ways to do that, one 

145
00:12:11.322 --> 00:12:17.310
way is what's called relaxation methods. 
Can we change these. 

146
00:12:17.310 --> 00:12:23.982
For something that is solvable. 
Something that we can actually do in our 

147
00:12:23.982 --> 00:12:28.337
computer. 
Meaning, can we relax our condition in 

148
00:12:28.337 --> 00:12:33.620
such a way that it's almost the same, but 
we can solve it. 

149
00:12:33.620 --> 00:12:40.027
The other way is to do what are called 
greedy algorithms, and that means, okay, 

150
00:12:40.027 --> 00:12:44.188
forget about finding all of them at the 
same time. 

151
00:12:44.188 --> 00:12:50.263
Give me the most important item, then the 
second, then the third, and so on. 

152
00:12:50.263 --> 00:12:55.740
So let us basically briefly present both 
of these techniques. 

153
00:12:55.740 --> 00:13:02.825
Relaxation methods are also some times 
called basis pursuit methods.This is our 

154
00:13:02.825 --> 00:13:09.680
problem, this is what we want to solve, 
relaxation methods will change this. 

155
00:13:09.680 --> 00:13:16.590
Look what I'm doing, I'm replacing the 
zero norm, or pseudo-norm by the one 

156
00:13:16.590 --> 00:13:20.792
norm. 
I'm now, no, not just counting the number 

157
00:13:20.792 --> 00:13:25.462
of knowns zero efficients, I'm basically 
counting with their magnitude. 

158
00:13:26.582 --> 00:13:34.180
Remember in the F0 norm we had a penalty 
which was something like this, flat. 

159
00:13:34.180 --> 00:13:37.803
Okay. 
So there was no penalty when you are zero 

160
00:13:37.803 --> 00:13:41.118
and the same penalty when you are non 
zero. 

161
00:13:41.118 --> 00:13:45.670
Here in the L1 norm. 
This was a type of penalty. 

162
00:13:45.670 --> 00:13:51.370
It increases with the magnitude. 
Now we are a bit surprised. 

163
00:13:51.370 --> 00:13:59.069
The beauty of this, is that under certain 
conditions, on D and there's certain 

164
00:13:59.069 --> 00:14:07.065
conditions on L, the level of sparsity. 
Basically we can guarantee that these two 

165
00:14:07.065 --> 00:14:11.263
are equivalent. 
This is a fantastic result. 

166
00:14:11.263 --> 00:14:18.860
It says that I can move from an np 
complete and solve other problem into an 

167
00:14:18.860 --> 00:14:22.373
l1 problem. 
This is a convex problem. 

168
00:14:22.373 --> 00:14:27.393
It's solvable. 
And sometimes obtain exactly here, the 

169
00:14:27.393 --> 00:14:32.510
solution I was looking here, but in a 
reasonable time. 

170
00:14:32.510 --> 00:14:39.358
Very beautiful theory behind this and. 
it's not the only case but it's a 

171
00:14:39.358 --> 00:14:44.521
powerful case where relaxation, actually 
we don't lose anything, by doing 

172
00:14:44.521 --> 00:14:48.306
relaxation. 
And this problem has been studied quite a 

173
00:14:48.306 --> 00:14:51.445
lot. 
As I say, it is called the basis pursuit, 

174
00:14:51.445 --> 00:14:53.300
is convex. 
So, we can solve. 

175
00:14:53.300 --> 00:14:57.510
There is a lot of algorithms in the 
literature to solve it. 

176
00:14:57.510 --> 00:15:03.432
And in the packages I mentioned that are 
free for you to download and to play with 

177
00:15:03.432 --> 00:15:09.068
sparse modeling, you have implementations 
and very efficient implementations of 

178
00:15:09.068 --> 00:15:12.422
these. 
And this actually has been the source of 

179
00:15:12.422 --> 00:15:15.490
a lot of research in the community 
because. 

180
00:15:15.490 --> 00:15:21.637
Now that we know that this is convex, and 
we're solving, under certain conditions, 

181
00:15:21.637 --> 00:15:26.774
the original problem. 
Let's try to solve it the best possible 

182
00:15:26.774 --> 00:15:33.427
way and this open the door to a lot of 
excellent research in this area, and I'm 

183
00:15:33.427 --> 00:15:39.322
just mentioning a few here. 
The literature is, is huge and you might, 

184
00:15:39.322 --> 00:15:43.938
actually. 
Know about it from your own research 

185
00:15:43.938 --> 00:15:47.429
area. 
Now the second say, so this is the 

186
00:15:47.429 --> 00:15:51.534
relaxation way and as I say, extremely 
powerful. 

187
00:15:51.534 --> 00:15:57.210
So you can pick to use one. 
The second one is the greedy approach, 

188
00:15:57.210 --> 00:16:04.022
the greedy approach is often called the 
matching pursuit and the idea is very, 

189
00:16:04.022 --> 00:16:08.040
very simple. 
Again, remember we have the signal. 

190
00:16:08.040 --> 00:16:12.681
That we are trying to approximate. 
That would be Y. 

191
00:16:12.681 --> 00:16:18.622
We have our dictionary D. 
And we are trying to find the sparsest 

192
00:16:18.622 --> 00:16:25.483
possible way to represent this signal. 
Now, what we do, and much in pursuit is a 

193
00:16:25.483 --> 00:16:31.984
standard technique in the statistics 
community and it was brought into signal 

194
00:16:31.984 --> 00:16:35.984
processing about twenty years ago or 
some, or so. 

195
00:16:35.984 --> 00:16:41.902
The basic idea is let's find, first find 
the most important atom in the 

196
00:16:41.902 --> 00:16:45.819
dictionary. 
So you go and travel the dictionary. 

197
00:16:45.819 --> 00:16:52.236
Let's observe what's happening here. 
You travel a dictionary and you find the 

198
00:16:52.236 --> 00:16:56.717
first one. 
So once again, you go, you try the 

199
00:16:56.717 --> 00:17:00.899
dictionary, and you find the most 
important one. 

200
00:17:00.899 --> 00:17:09.263
What do I mean by the most important one? 
The one that minimizes the D alpha - 1^2. 

201
00:17:09.263 --> 00:17:15.380
So. 
We have the'd' alpha minus'y'. 

202
00:17:15.380 --> 00:17:19.478
Square, and this no, this is alpha, this 
was cut. 

203
00:17:19.478 --> 00:17:25.181
This is no, this no, Y is no. 
This is d, this is y, and supports one. 

204
00:17:25.181 --> 00:17:31.775
So you try the first one, you try the 
second one, you try the third one, you 

205
00:17:31.775 --> 00:17:36.320
try the fourth one. 
The one that does this the best. 

206
00:17:36.320 --> 00:17:42.322
Actually because this is the method we're 
using the, this one will be the one that 

207
00:17:42.322 --> 00:17:45.689
maximizes the inner product with the 
signal Y. 

208
00:17:45.689 --> 00:17:50.520
Basically which is the one which is most 
parallel to the signal Y. 

209
00:17:50.520 --> 00:17:53.383
That's going to be this one. 
Now I keep it. 

210
00:17:53.383 --> 00:17:57.663
I cannot give it back. 
In contrast with what we saw in the 

211
00:17:57.663 --> 00:18:03.493
previous slide that we basically tried L 
equal one, then tried L equal two, then 

212
00:18:03.493 --> 00:18:06.915
tried L equal three. 
This I'm forced to keep. 

213
00:18:06.915 --> 00:18:10.321
That's why the technique is called 
greedy. 

214
00:18:10.321 --> 00:18:15.510
Once you select it, you keep it. 
So now I selected this, I keep it. 

215
00:18:15.510 --> 00:18:21.793
I keep it and there is still an error 
here, if the error is below what I wanted 

216
00:18:21.793 --> 00:18:27.081
I stop, if not I have an error. 
Now the next step is pick the one that 

217
00:18:27.081 --> 00:18:30.876
helps you to make that error the smallest 
possible. 

218
00:18:30.876 --> 00:18:35.415
So, for the first one was Y, what we are 
trying to approximate. 

219
00:18:35.415 --> 00:18:39.656
For the second one, we're trying to 
approximate the error. 

220
00:18:39.656 --> 00:18:45.162
If we approximate the error, we add that 
atom to our collection, and we are 

221
00:18:45.162 --> 00:18:50.256
starting to shrink the error. 
So the next time you keep this one. 

222
00:18:50.256 --> 00:18:56.466
And basically you go to the one that has 
the best fit to the error so far. 

223
00:18:56.466 --> 00:19:01.700
So again, you try all of them. 
And you keep the one that minimizes the 

224
00:19:01.700 --> 00:19:04.710
error. 
If you see that again you try all of 

225
00:19:04.710 --> 00:19:08.475
them. 
And we keep the one that minimizes the 

226
00:19:08.475 --> 00:19:11.749
error. 
Now, please note I've kept the first one 

227
00:19:11.749 --> 00:19:14.571
and I'm now keeping the second one. 
Okay. 

228
00:19:14.571 --> 00:19:20.121
So I kept the first one and I kept the 
second one Now I have a new error because 

229
00:19:20.121 --> 00:19:24.513
I'm using two items. 
I have a new error and I'm going to pick 

230
00:19:24.513 --> 00:19:28.760
the third one that basically approximates 
that error again. 

231
00:19:28.760 --> 00:19:32.720
And every time all that you're doing is 
inner products. 

232
00:19:32.720 --> 00:19:38.629
To approximate those errors and now the 
algorithm is doable because for every one 

233
00:19:38.629 --> 00:19:43.890
they have to try, you basically try N 
times, you are picking one at a time 

234
00:19:43.890 --> 00:19:49.800
instead of picking this L at a time and 
interchanging and putting back and trying 

235
00:19:49.800 --> 00:19:52.899
new ones. 
The one you pick, you stay with it. 

236
00:19:52.899 --> 00:19:58.737
And now this algorithm is very efficient, 
it's just a bunch of inner products all 

237
00:19:58.737 --> 00:20:02.340
the time. 
And you stop when you go to a reasonable 

238
00:20:02.340 --> 00:20:05.800
error, an error that satisfies your 
requirements. 

239
00:20:05.800 --> 00:20:12.647
Now, this is what basically I wrote here, 
you stop when your error is satisfied. 

240
00:20:12.647 --> 00:20:19.319
There are some variations of this. 
The most important one is that every time 

241
00:20:19.319 --> 00:20:25.990
that you pick a new atom, because it's 
the most important one, you take again 

242
00:20:25.990 --> 00:20:30.091
your signal Y. 
And you project into all the selected 

243
00:20:30.091 --> 00:20:33.340
atoms. 
In other words, when I pick the first 

244
00:20:33.340 --> 00:20:36.740
atom. 
I have a certain coefficient alpha here. 

245
00:20:36.740 --> 00:20:41.122
When I picked the second atom, I got a 
certain coefficient. 

246
00:20:41.122 --> 00:20:46.487
Now, these two are already selected. 
So, may be I can take the signal and 

247
00:20:46.487 --> 00:20:52.305
change the actual value here and the 
actual value here to get a better error. 

248
00:20:52.305 --> 00:20:58.274
If I'm already selecting it, why to stay 
with the actual coefficient that I got 

249
00:20:58.274 --> 00:21:01.350
when I selected it? 
I, I'll to change it. 

250
00:21:01.350 --> 00:21:07.503
So, every time you change, you do a 
projection onto the sub space of all the 

251
00:21:07.503 --> 00:21:12.097
selected atoms. 
Again that's a least course proceeded to 

252
00:21:12.097 --> 00:21:14.230
very fast to implement. 
So. 

253
00:21:14.230 --> 00:21:20.089
Orthogonal matching pursuit, you select 
the best atom so far, you project on to 

254
00:21:20.089 --> 00:21:24.221
the best selected atoms, that gives you 
an error so far. 

255
00:21:24.221 --> 00:21:30.081
Next step you select, once again, the 
best atom to minimize that error that you 

256
00:21:30.081 --> 00:21:35.640
are getting so far, you project onto all 
the atoms and then you keep going. 

257
00:21:35.640 --> 00:21:41.050
So that's a very simple algorithm. 
Again, it's implemented in some of the 

258
00:21:41.050 --> 00:21:46.978
software that we give you the link for. 
So, the basic idea is now we have 

259
00:21:46.978 --> 00:21:53.976
techniques to solve these problems. 
We have techniques which are basically of 

260
00:21:53.976 --> 00:21:57.487
several classes. 
The greedy algorithms that I just 

261
00:21:57.487 --> 00:22:00.089
mentioned to you, that you go one at a 
time. 

262
00:22:00.089 --> 00:22:03.720
There are variations of that. 
There are many, many variations. 

263
00:22:03.720 --> 00:22:06.625
Some people say, why don't we pick two at 
a time? 

264
00:22:06.625 --> 00:22:09.409
I do have, sent computation in time to do 
that. 

265
00:22:09.409 --> 00:22:13.584
And things like that or there are 
Orthogonal Matching Pursuit that I 

266
00:22:13.584 --> 00:22:16.772
mentioned. 
They are the relaxation algorithms. 

267
00:22:16.772 --> 00:22:20.092
The literature is huge in that topic as 
well. 

268
00:22:20.092 --> 00:22:26.142
And the basic idea was to relax this by 
something that will give us hopefully the 

269
00:22:26.142 --> 00:22:31.750
same result what we're looking for. 
And there hybrid algorithms that do both 

270
00:22:31.750 --> 00:22:36.030
of them at the same time in some very 
clever combinations. 

271
00:22:36.030 --> 00:22:41.619
Now why should I work. 
Because there is theory behind them that 

272
00:22:41.619 --> 00:22:47.439
tells us that sometimes the relaxation, 
although it's a relaxation, it actually 

273
00:22:47.439 --> 00:22:51.917
produces the right results. 
There are theories that gives us 

274
00:22:51.917 --> 00:22:56.468
conditions for that. 
There is theory that gives us conditions 

275
00:22:56.468 --> 00:23:00.125
for the greedy. 
Although it's greedy, sometimes it 

276
00:23:00.125 --> 00:23:05.124
produces the right result. 
For example, if the matrix D is a Fourier 

277
00:23:05.124 --> 00:23:11.280
matrix, you got the right results there. 
So there are ways of doing this kind of. 

278
00:23:11.280 --> 00:23:17.084
Transform this basically l0 type of 
approach, incompletely dissolvable and 

279
00:23:17.084 --> 00:23:23.124
there is a beautiful theory that there is 
a, when relaxation already, actually 

280
00:23:23.124 --> 00:23:27.360
gives us the best, the result that we 
were looking for. 

281
00:23:27.360 --> 00:23:32.795
If our problem, for example the D matrix 
doesn't hold those conditions, in 

282
00:23:32.795 --> 00:23:37.337
practice we still use one of these two 
classes of algorithms. 

283
00:23:37.337 --> 00:23:40.614
Why? 
Because solving the real problem is not 

284
00:23:40.614 --> 00:23:44.188
doable. 
It's impossible, so the best we can do is 

285
00:23:44.188 --> 00:23:50.219
do either a relaxation or a greedy. 
And if we are in the right conditions we 

286
00:23:50.219 --> 00:23:53.421
get the exact solution we were looking 
for. 

287
00:23:53.421 --> 00:23:58.140
If not hopefully we get a very good 
approximation of that. 

288
00:23:58.140 --> 00:24:02.830
So where are we so far? 
We started by image denoising and looking 

289
00:24:02.830 --> 00:24:07.593
for the need of modeling. 
We derived sparse modeling as one way of 

290
00:24:07.593 --> 00:24:11.563
doing a model of images that will help us 
in denoising. 

291
00:24:11.563 --> 00:24:17.264
Now we discussed some of the problems and 
basically we saw how to solve, how to 

292
00:24:17.264 --> 00:24:21.233
compute the alpha that will give us the 
representation. 

293
00:24:21.233 --> 00:24:26.357
The next step is the dictionary, and 
that's what we are going to do in the 

294
00:24:26.357 --> 00:24:28.594
next video. 
Thank you very much. 

295
00:24:28.594 --> 00:24:30.615
I'm looking forward to that. 