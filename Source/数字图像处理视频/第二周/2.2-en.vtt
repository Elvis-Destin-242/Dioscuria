WEBVTT

1
00:00:00.000 --> 00:00:04.776
We're now ready to describe each of one 
of the blocks that built an image 

2
00:00:04.776 --> 00:00:07.810
compression algorithm and in particular, 
a JPEG. 

3
00:00:07.810 --> 00:00:11.554
And as we say, we are going to start with 
a symbol encoder. 

4
00:00:11.554 --> 00:00:16.266
After we have basically done everything, 
we have done the transform and the 

5
00:00:16.266 --> 00:00:21.108
quantization that we are going to explain 
in detail, we want to further compress. 

6
00:00:21.108 --> 00:00:25.626
So, quantization will provide us on 
compression that we want to further 

7
00:00:25.626 --> 00:00:28.596
compress. 
But we need to save whatever we have 

8
00:00:28.596 --> 00:00:31.590
produced here. 
We need to save with no error. 

9
00:00:31.590 --> 00:00:34.797
But we're still going to see how we can 
compress. 

10
00:00:34.797 --> 00:00:38.770
So, why can we achieve that? 
The basic idea is very simple. 

11
00:00:38.770 --> 00:00:44.080
And, of course, you can not teach image 
processing without showing that first, at 

12
00:00:44.080 --> 00:00:47.609
least once, 
this famous image that is called Lena. 

13
00:00:47.609 --> 00:00:51.279
That it, it used to be used a lot in 
image processing. 

14
00:00:51.279 --> 00:00:56.417
But now, because of the abundance of 
images, many other images are used. 

15
00:00:56.417 --> 00:01:00.821
But I want to show it at least once 
during these, these nine weeks. 

16
00:01:00.821 --> 00:01:04.858
And we might see the same image again a 
few more times. 

17
00:01:04.858 --> 00:01:10.804
So, the basic idea is that some pixels, 
some gray values in the image appear more 

18
00:01:10.804 --> 00:01:13.080
than others. 
And let's see that. 

19
00:01:13.080 --> 00:01:16.640
Here on the right, we have what's called 
a histogram. 

20
00:01:16.640 --> 00:01:22.272
The histogram basically counts how many 
times a particular gray value appears in 

21
00:01:22.272 --> 00:01:25.609
the image. 
For example, the pixel value, the pixel 

22
00:01:25.609 --> 00:01:30.198
with gray values, let's say, around 47, 
appears this amount of times. 

23
00:01:30.198 --> 00:01:35.692
So, we go around the image, we count how 
many times I see the number 47 and this 

24
00:01:35.692 --> 00:01:39.933
is the amount of times. 
Then I can go and see, for example, 200, 

25
00:01:39.933 --> 00:01:44.940
and I see this, this amount of time, 
about 700 times appears in the image. 

26
00:01:44.940 --> 00:01:49.978
So, we see that there are some pixels 
that appear a lot, there are some pixels, 

27
00:01:49.978 --> 00:01:52.726
let's say, zero or one don't appear at 
all. 

28
00:01:52.726 --> 00:01:57.568
And the basic idea that we are going to 
exploit is that we're going to try to 

29
00:01:57.568 --> 00:02:02.410
give to pixels, to pixels that appear a 
lot, very short course, and to pixel 

30
00:02:02.410 --> 00:02:07.579
values that don't appear so much, we're 
going to try to give very long cause or 

31
00:02:07.579 --> 00:02:10.480
longer course. 
Let's see how we do that. 

32
00:02:10.480 --> 00:02:16.003
Let's see an example of the use of this. 
So here, we have an image, and this is 

33
00:02:16.003 --> 00:02:20.429
actually the image that we saw in the 
previous video, the geometric image. 

34
00:02:20.429 --> 00:02:25.352
This is a particular image that has, 
there's only four different gray values 

35
00:02:25.352 --> 00:02:32.043
in that image, and they appear, and so 
the pixels, pixels with gray value 87 

36
00:02:32.043 --> 00:02:37.518
appear in 1/4 on the image, so 25%,, 25% 
or probabilty of 0.25. 

37
00:02:37.518 --> 00:02:44.035
And we also have 128 appearing almost 
half of the time in the image, and we 

38
00:02:44.035 --> 00:02:50.813
also have 186 and 255 appears very, very 
few times, everybody else just doesn't 

39
00:02:50.813 --> 00:02:55.910
appear in the image at all. 
So, the standard way to represent this 

40
00:02:55.910 --> 00:03:01.629
image is we have an array and for every 
pixel, we have a binary code with eight 

41
00:03:01.629 --> 00:03:04.815
bits. 
So, we are going to basically represent 

42
00:03:04.815 --> 00:03:10.172
87 with its binary representation. 
We are going to represent 128 with its, 

43
00:03:10.172 --> 00:03:15.963
its own binary representation and so on. 
So, at the end, we are basically spending 

44
00:03:15.963 --> 00:03:19.717
eight bits per pixel if we don't 
compress. 

45
00:03:19.717 --> 00:03:27.063
But let's assume that somehow I managed 
to basically construct a code that is 

46
00:03:27.063 --> 00:03:31.738
shown here, 
in a way that 87 is represented by the 

47
00:03:31.738 --> 00:03:37.373
binary code 01, 
128 is represented by the binary code 

48
00:03:37.373 --> 00:03:42.462
one, 
186 by the binary code 000 and 255 by the 

49
00:03:42.462 --> 00:03:47.330
binary code 001. 
That means that, for example, if I want 

50
00:03:47.330 --> 00:03:52.640
to, if I want to store or send the pixels 
87, 87, 

51
00:03:52.640 --> 00:03:55.101
255, 
128. 

52
00:03:55.101 --> 00:04:02.608
Instead of writing all these then, this 
again, for 255, this and for 128, this, 

53
00:04:02.608 --> 00:04:10.320
which will take me basically, 4 times 8, 
32 bits and I'm going to write it in the 

54
00:04:10.320 --> 00:04:13.714
following way. 
I'm going to say, okay, 

55
00:04:13.714 --> 00:04:18.260
87 is 01. 
87 again, 01. 

56
00:04:18.260 --> 00:04:21.346
255, 
001. 

57
00:04:21.346 --> 00:04:27.607
128, 1. 
So, basically it took me two for seven, 

58
00:04:27.607 --> 00:04:35.310
eight bits instead of 32 to represent the 
same stream the same form of gray values. 

59
00:04:35.310 --> 00:04:42.313
Now, the decoder has this table, so the 
decoder will see 01, we'll go into that 

60
00:04:42.313 --> 00:04:48.201
pixel position and we'll put an 87. 
We'll see 01, we'll put again an 87. 

61
00:04:48.201 --> 00:04:53.159
We'll see 001, we say, oh, I need to put 
a 255, and so on. 

62
00:04:53.159 --> 00:04:59.435
And so, we have achieved compression. 
Let's see exactly how much compression we 

63
00:04:59.435 --> 00:05:02.921
can achieve. 
So basically, we do a very simple 

64
00:05:02.921 --> 00:05:06.485
computation. 
We say, okay, with probability zero 

65
00:05:06.485 --> 00:05:08.460
point, 
sorry, 

66
00:05:10.120 --> 00:05:22.080
with probability 0.25, 
I'm going to have length of 01, 

67
00:05:22.080 --> 00:05:27.625
a length of, of two. 
That's my, my code, 01. 

68
00:05:27.625 --> 00:05:36.189
With probability 0.47, I'm going to have 
length of 1 plus 0.47 times one. 

69
00:05:36.189 --> 00:05:45.301
With probability 0.25, so for one quarter 
of the image, I'm going to have length 

70
00:05:45.301 --> 00:05:55.326
three. 
And with very small probability, I'm also 

71
00:05:55.326 --> 00:06:04.217
going to have length three. 
And this is equal to 1 60, 

72
00:06:04.217 --> 00:06:10.240
1.81. 
Okay. 

73
00:06:10.240 --> 00:06:18.187
So, instead eight bits per pixel, I now 
have 1.81 on average on my whole image. 

74
00:06:18.187 --> 00:06:26.443
I basically, I have save lots of pixels. 
Okay, I have gone from eight to basically 

75
00:06:26.443 --> 00:06:29.526
1.81. 
And that's a, that's a compression of 

76
00:06:29.526 --> 00:06:32.648
over four times, and that's what we have 
done. 

77
00:06:32.648 --> 00:06:37.850
Basically, what we did is we have 
exploited the fact that some of the pixel 

78
00:06:37.850 --> 00:06:43.053
values appear more often than others, and 
then I have to give shorter codes. 

79
00:06:43.053 --> 00:06:47.214
I can give shorter codes. 
So, how we do that, we do that using 

80
00:06:47.214 --> 00:06:51.931
something which is called Huffman coding. 
And I'm going to explain next how to do 

81
00:06:51.931 --> 00:06:55.778
it. 
So, lets just look once again in this 

82
00:06:55.778 --> 00:07:01.335
particular example, the best way to 
explain Huffman coding is just to use an 

83
00:07:01.335 --> 00:07:06.380
example, and I'm going to that next. 
So, let's assume that we have five 

84
00:07:06.380 --> 00:07:09.743
different symbols, six different symbols, 
sorry. 

85
00:07:09.743 --> 00:07:15.455
And I order them, the first thing they 
have to do is order them with, according 

86
00:07:15.455 --> 00:07:19.654
to the probability. 
The highest probability first and the 

87
00:07:19.654 --> 00:07:24.737
lowest probability at the end. 
And if some probabilities are equal, it 

88
00:07:24.737 --> 00:07:30.410
doesn't matter which one we put first. 
So, for example, this symbol a2 might be 

89
00:07:30.410 --> 00:07:34.830
the pixel value 100. 
The symbol a6 might be the pixel value 

90
00:07:34.830 --> 00:07:37.040
77, and so on. 
So, I'm going to add, 

91
00:07:37.040 --> 00:07:42.860
basically I'm going to list the prob, the 
symbols here and the probabilities here. 

92
00:07:42.860 --> 00:07:47.060
And from the bottom, I start to add. 
I take the two lowest. 

93
00:07:47.060 --> 00:07:53.943
And basically, I add their probabilities. 
So, I go from 0.04 plus 0.06 and that's 

94
00:07:53.943 --> 00:07:58.213
probability one, 0.1. 
And then, I do the list again. 

95
00:07:58.213 --> 00:08:04.400
So, none of these has changed, the only 
one that changed is the last one. 

96
00:08:04.400 --> 00:08:09.280
See what's happening here. 
What's happening here is with probability 

97
00:08:09.280 --> 00:08:16.600
0.4, I can get a2, with probability 0.3, 
I can get a6, with probability 0.1, I can 

98
00:08:16.600 --> 00:08:20.603
get a1, 
with probability 0.1, I can get symbol 

99
00:08:20.603 --> 00:08:25.229
a4, and with probability 0.1, I can get 
either a3 or a5. 

100
00:08:25.229 --> 00:08:31.013
I am going to have to do something to 
distinguish which one is it, is it a3 or 

101
00:08:31.013 --> 00:08:36.440
a5? And that's going to come in the next 
slide how we distinguish them. 

102
00:08:36.440 --> 00:08:38.834
Now, we do it, 
we do this again. 

103
00:08:38.834 --> 00:08:42.343
We take the two lowest and then we add 
them. 

104
00:08:42.343 --> 00:08:47.840
And now, because it became 0.2, remember, 
we have to reorder all the time. 

105
00:08:47.840 --> 00:08:52.951
So, let me just describe that again. 
With 0.4, we can get a2. 

106
00:08:52.951 --> 00:08:58.155
With 0.3, we can get a6. 
Now, with 0.2, we can get either one of 

107
00:08:58.155 --> 00:09:00.984
these two, 
which is a4, a3 or a5. 

108
00:09:00.984 --> 00:09:07.192
I don't know which one but I know with 
0.2 probability is one of them. 

109
00:09:07.192 --> 00:09:12.304
And then, we have 0.1 again. 
We do that once more, once more, 

110
00:09:12.304 --> 00:09:16.594
actually, two more times. 
We have the two lowest. 

111
00:09:16.594 --> 00:09:21.410
We get this and then we, once again, add 
it to lowest, and we get this. 

112
00:09:21.410 --> 00:09:26.393
And once again, we, every time we add it 
to lowest and we move into the next 

113
00:09:26.393 --> 00:09:32.020
column, we'll reorder everything. 
When we end up with only two, we stop. 

114
00:09:32.020 --> 00:09:37.826
And we're going to see why we stop. 
Now, it's time to give the code. 

115
00:09:37.826 --> 00:09:46.657
So, basically, what I have done here, I 
basically redraw the graph that we have 

116
00:09:46.657 --> 00:09:51.478
in the previous slide. 
And so, this is the first column, this is 

117
00:09:51.478 --> 00:09:57.544
the second column, and the third column, 
the fourth column, and the fifth column. 

118
00:09:57.544 --> 00:10:01.199
And now, I'm going to go backwards, 
assigning codes. 

119
00:10:01.199 --> 00:10:05.895
So, basically to 0.6, I assigned zero. 
And to 0.4, I assigned one. 

120
00:10:05.895 --> 00:10:10.125
Before we go backwards, let me tell you 
what's happening. 

121
00:10:10.125 --> 00:10:15.562
When the decoder is going to see a zero, 
it's going to know that it's from the 

122
00:10:15.562 --> 00:10:20.622
group that led to this 0.6. 
When you see sub one, it's going to know 

123
00:10:20.622 --> 00:10:24.851
that it's from the group that leads to, 
lead to this 0.4. 

124
00:10:24.851 --> 00:10:30.440
Then we're going to have to keep adding 
bits to say which one in that group. 

125
00:10:30.440 --> 00:10:36.760
And that's why we keep going backwards. 
So, we basically propagate back the zero. 

126
00:10:36.760 --> 00:10:42.417
And we actually, the 0.6 came from 
0.30.3, plus 0.3. 

127
00:10:42.417 --> 00:10:49.958
So, we add a zero and a one. 
So now we, of course, propagated 0.4 here 

128
00:10:49.958 --> 00:10:56.132
and basically assigned it to this that 
came to this column, without adding any 

129
00:10:56.132 --> 00:11:00.107
numbers. 
So, it keeps propagating without adding 

130
00:11:00.107 --> 00:11:04.420
any bits. 
So now, all the symbols that lead to this 

131
00:11:04.420 --> 00:11:09.580
group and we're going to see, it was 
actually only one symbol, 

132
00:11:09.580 --> 00:11:16.843
we have this code and all the symbols 
that propagated to this probability. 

133
00:11:16.843 --> 00:11:22.732
We have symbols that start, we have codes 
that start from 01. 

134
00:11:22.732 --> 00:11:29.211
So now, we keep propagating. 
We propagate the one to the one, the 00 

135
00:11:29.211 --> 00:11:33.824
to the 00. 
01, we have to split because this 0.3 

136
00:11:33.824 --> 00:11:38.536
probability came from the addition of 
these two. 

137
00:11:38.536 --> 00:11:43.126
And then, we have 010, 011. 
And then we keep going. 

138
00:11:43.126 --> 00:11:50.789
The, we propagate the one to the one, the 
zero to the zero, this 010, which has a 

139
00:11:50.789 --> 00:11:56.293
code 011 propagates here. 
And this actually has to split. 

140
00:11:56.293 --> 00:12:02.135
Every time we split because when we were 
computing it, we were adding. 

141
00:12:02.135 --> 00:12:05.776
So, when we're going back, we were 
splitting. 

142
00:12:05.776 --> 00:12:10.518
Every time we are splitting, we have to 
add zero and one. 

143
00:12:10.518 --> 00:12:17.037
And so, we keep going, we keep going, and 
we get this result that says that the 

144
00:12:17.037 --> 00:12:23.426
symbol 01 is going to have a code, one. 
The symbol 06 is going to have a code 00 

145
00:12:23.426 --> 00:12:25.684
and so on. 
So, this is the code, 

146
00:12:25.684 --> 00:12:30.803
it's a variable length code. 
Not every symbol is represented by a code 

147
00:12:30.803 --> 00:12:35.169
of the same length. 
It actually has a different length and 

148
00:12:35.169 --> 00:12:40.965
the length depends on the probability. 
The higher the probability, the shorter 

149
00:12:40.965 --> 00:12:44.653
the code. 
And that's very easy to see because the 

150
00:12:44.653 --> 00:12:50.148
low probabilities were added, added, 
added, added and then, when we're going 

151
00:12:50.148 --> 00:12:53.160
back we have to split, split, split, 
split. 

152
00:12:53.160 --> 00:12:58.769
And remember, every time we split, we 
have to add a symbol, and we have to add 

153
00:12:58.769 --> 00:13:03.493
a bit for that symbol. 
So, basically the code becomes longer and 

154
00:13:03.493 --> 00:13:07.115
longer. 
And we can actually go and compute, once 

155
00:13:07.115 --> 00:13:12.704
again, the average length of these. 
So, to repeat the process, we basically 

156
00:13:12.704 --> 00:13:16.352
order the symbols according to the 
probability, 

157
00:13:16.352 --> 00:13:20.855
that's what we did here. 
We start adding low probabilities. 

158
00:13:20.855 --> 00:13:25.590
So, we add these two, we got this, 
we reorder and we keep going. 

159
00:13:25.590 --> 00:13:31.645
It's a recursive process so if you know 
how to go from one column to the next, 

160
00:13:31.645 --> 00:13:36.040
you know how to go all the way. 
And then, we come back. 

161
00:13:36.040 --> 00:13:41.763
There is a couple of things that are very 
important to observe here. 

162
00:13:41.763 --> 00:13:47.067
The code that you get in this way is 
what's called prefix-free. 

163
00:13:47.067 --> 00:13:51.360
And that's very, very important. 
Look at this column. 

164
00:13:51.360 --> 00:13:59.148
When you see a one, when the decoder sees 
a one, it knows that it has to go here, 

165
00:13:59.148 --> 00:14:03.247
to a2. 
When is this a zero, it basically says, 

166
00:14:03.247 --> 00:14:10.170
okay, I have to wait for the next symbol. 
But there is no code here, which is a 

167
00:14:10.170 --> 00:14:16.299
full prefix of any other code. 
And that's very important, otherwise, the 

168
00:14:16.299 --> 00:14:22.851
decoder won't be able to reconstruct. 
Let's give an example of something which 

169
00:14:22.851 --> 00:14:27.302
is not a prefix code. 
Let's say that the symbol a1 is 

170
00:14:27.302 --> 00:14:33.854
represented by the code one, the symbol 
a2 is represented by the code zero, and 

171
00:14:33.854 --> 00:14:37.550
the symbol a3 is represented by the code 
01, 

172
00:14:37.550 --> 00:14:44.000
okay? 
So, when the decoder sees 01, 

173
00:14:44.000 --> 00:14:49.060
basically there are two possibilities. 
It's either a3, 

174
00:14:49.060 --> 00:14:57.955
let's say, pixel value 100 or is a2 
followed by a1. 

175
00:14:57.955 --> 00:15:04.891
Let's say, pixel zero and pixel 255. 
So, there is a lot of ambiguity in the 

176
00:15:04.891 --> 00:15:09.167
reconstruction. 
We don't know which one it is. 

177
00:15:09.167 --> 00:15:15.533
And this is because this code is not 
what's called prefix-free. 

178
00:15:15.533 --> 00:15:21.044
This guy is a subset of this guy. 
That doesn't happen here. 

179
00:15:21.044 --> 00:15:27.847
Let's just clear these annotations. 
None of these codes, 

180
00:15:27.847 --> 00:15:33.052
so for example, the 00, you don't find it 
anywhere here. 

181
00:15:33.052 --> 00:15:38.059
So, when the decoder sees a one, 
knows what to do. 

182
00:15:38.059 --> 00:15:43.000
It puts the pixel value of A2. 
When it sees a zero. 

183
00:15:43.000 --> 00:15:49.364
Is it, oh, I'll have to wait, because 
there are many codes that starts with 

184
00:15:49.364 --> 00:15:53.374
zero. 
If this is a zero, another zero and I'll 

185
00:15:53.374 --> 00:16:00.697
say, oh, I'm done, I basically have a6. 
If, let's say, it's, so, the decoder 

186
00:16:00.697 --> 00:16:07.410
receives zero and then one, says, oh, I 
have to wait, because there are numerous 

187
00:16:07.410 --> 00:16:11.391
codes that start with 01. 
But then, it receives an, a one. 

188
00:16:11.391 --> 00:16:14.795
that says, oh, I'm done. 
I don't have to keep waiting. 

189
00:16:14.795 --> 00:16:18.799
Because there's nothing. 
There's no ambiguity here. 

190
00:16:18.799 --> 00:16:22.603
And that's very important, a Huffman code 
by construction. 

191
00:16:22.603 --> 00:16:25.806
Look how we basically, split, and split, 
and split. 

192
00:16:25.806 --> 00:16:29.010
And by construction, this is a 
prefix-free code. 

193
00:16:29.010 --> 00:16:33.189
Now, how do we know that this is an 
optimal code? 

194
00:16:33.189 --> 00:16:37.630
How do we, how do we know that with this 
set of probabilities, 

195
00:16:37.630 --> 00:16:43.450
the process, the procedure I just 
explained achieved the lowest possible 

196
00:16:43.450 --> 00:16:48.746
the, the lowest possible length, the 
shortest possible length of a code on 

197
00:16:48.746 --> 00:16:52.011
average. 
So, that's a theorem we have to prove, 

198
00:16:52.011 --> 00:16:57.670
we're not going to do that in this class, 
but as a theorem that, that shows the 

199
00:16:57.670 --> 00:17:02.314
Huffman coding basically achieves the 
shortest possible length. 

200
00:17:02.314 --> 00:17:07.973
Now, you might wonder what length can 
Huffman coding achieve or any basically 

201
00:17:07.973 --> 00:17:12.616
encoder of this tile. 
So, the basic idea is if I give you a set 

202
00:17:12.616 --> 00:17:15.570
of probabilities, 
how much can I compress? 

203
00:17:15.570 --> 00:17:21.234
How much, I go, and before I construct a 
Huffman code, I want to know, is it worth 

204
00:17:21.234 --> 00:17:25.393
the effort of constructing the code? 
Will I compress a lot? 

205
00:17:25.393 --> 00:17:30.699
And the way, the way to do that, is 
basically you compute what's called the 

206
00:17:30.699 --> 00:17:33.080
entropy. 
And the entropy, 

207
00:17:33.080 --> 00:17:36.597
which is written by H, 
H is entropy, 

208
00:17:36.597 --> 00:17:55.713
[SOUND] is the sum over all the symbols 
with a minus here, I'm going to explain 

209
00:17:55.713 --> 00:18:05.254
in a second, of the probability, of every 
symbol log base 2 of the probability of 

210
00:18:05.254 --> 00:18:12.061
every symbol. 
So, it's basically sum, 0.4 times log 

211
00:18:12.061 --> 00:18:18.763
base two of 0.4 plus 0.3 times log base 
two of 0.3 and so on. 

212
00:18:18.763 --> 00:18:24.409
So, this is actually, at the end, a 
positive number because all probabilities 

213
00:18:24.409 --> 00:18:30.886
are below one and therefore, the log of 
the probabilities are negative numbers 

214
00:18:30.886 --> 00:18:34.788
and then with the negative, we get the 
positive. 

215
00:18:34.788 --> 00:18:40.850
And Shannon's First theorem basically 
says, that we can achieve basically, 

216
00:18:40.850 --> 00:18:45.350
asymptotically, we can achieve this code 
length on average. 

217
00:18:45.350 --> 00:18:51.439
So, this is basically what's achievable 
with these types of codes, like Huffman 

218
00:18:51.439 --> 00:18:56.452
code. 
We basically can compute this number and 

219
00:18:56.452 --> 00:19:00.310
say, oh, that's going to be my average 
code length. 

220
00:19:00.310 --> 00:19:05.556
My expected average code length. 
And the, it's actually not so hard to 

221
00:19:05.556 --> 00:19:10.952
understand where this formula comes from. 
We used a similar computation with the 

222
00:19:10.952 --> 00:19:14.662
previous example. 
Here, the probability is basically the 

223
00:19:14.662 --> 00:19:19.892
probability of the symbol appearing. 
And this is actually the length of the 

224
00:19:19.892 --> 00:19:23.840
code that is going to be used for that 
symbol. 

225
00:19:23.840 --> 00:19:30.937
So basically, what this is saying is that 
the expected length of that code is the 

226
00:19:30.937 --> 00:19:36.530
log of the probability. 
And why it's only the expected is because 

227
00:19:36.530 --> 00:19:42.461
you cannot have a code which is 3.2 long. 
You have to send entire bits. 

228
00:19:42.461 --> 00:19:46.614
Either you send three bits or you send 
four bits. 

229
00:19:46.614 --> 00:19:51.189
And that's why this is on average the 
expectation, 

230
00:19:51.189 --> 00:19:57.911
what you expect to be the code length. 
But this basically gives you an idea of 

231
00:19:57.911 --> 00:20:01.850
what encoders at Huffman coding can 
achieve. 

232
00:20:01.850 --> 00:20:07.665
So, now that we know how to encode the 
bits that we are producing, we are ready 

233
00:20:07.665 --> 00:20:11.245
to go into the next blocks of image 
compression. 