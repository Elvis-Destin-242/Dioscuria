Hello and welcome back. 
One of the fun things of this week is that we are learning a lot of background 
material. While the background material is 
important for image processing, it's important for many other disciplines. 
For example, in the previous video we learned about level sets. 
Level sets are important in image processing to the curve evolution like 
active contours as we have seen numerous examples last week. 
We saw a few this week and we are going to see more even in the next coming 
videos. Another topic that is very important in 
mathematics and is also important in image processing is the calculus of 
variations. And that's what we're going to be talking 
right now in this video. And then in the next video we're going to 
see an example of that for image denoiseing and image enhancement. 
So what is calculus of variations? The idea is very simple, because it's an 
extension of finding extrema of functions. 
But now we are going to be finding extrema of functionals. 
And what do I mean by that? We're going to have images like you. 
And. It's derivatives. 
So this is going to be an image for us. And we're going to be looking at 
minimizing a function of the image. Might be the image is square, might be 
the gradient of the image, might be the Laplacian of the image. 
So we're going to be trying to find what's the image that minimizes that 
function in the same way that we tried to find, what's the point that minimizes a 
given function. For example, if I draw this function then 
you say oh, this is the point that minimizes it. 
But here, U is by itself what we are looking for. 
So we are not looking for a coordinate, we are looking for a function that 
minimizes this. This is going to be very clear even in 
the next slide with an example. And we are going to make all the 
connection of the minimization of function X with the minimization of 
functions. Now, we're going to discuss in a couple 
of minutes that a condition for U to be a minimizer of this is to hold. 
This equation which is called the Euler-Lagrange equation and this is the 
type of partial differential equation that we are going to be solving in image 
processing. Let me illustrate a bit more about this 
and then how we came to this equation which is completely parallel to the 
minimization of regular functions that we are away from regular standards in 
calculus. So let's just illustrate, first of all, 
what do I mean by a functional. Let's assume that I'm giving two points. 
And I am asking you what's the curve connecting these two points that has the 
minimum length. So I am asking you for the function of 
that curve, the minimizer is going to be a function, not a point. 
The length of. Any curve, absolutely. 
Any curve connecting these two points is basically written here. 
The curve is parameterized as X Ux, we know about that, that's the particular 
case of that parameterization for basically any curve that goes. 
And connect those points. the length is. 
As we know, the derivative of the first coordinate squared so that's one squared 
derivative of the second coordinate squared. 
So it's this function that we are going to try to optimize. 
We already know what a function that minimizes the length between two points. 
It's a straight line. But let's see how we get that. 
You see calculus offers variations, so I'm trying to find U connecting these two 
points. This is the condition for a function to 
be a minimizer of this function. So it's that integral over a function of 
functions. We haven't arrived at it yet, but we're 
going to discuss it in a few slides that this is what happened. 
So F is square root of one plus UX squared. 
So if you just do the numbers and takes, you take this and do the derivative 
according to U minus the derivative that is written here, you get this expression. 
So this expression is nothing else than applying this to f as written here. 
Now this equal to zero is a condition for the particular U we are looking for to 
solve the problem that we are searching for, meaning the minimizer of the length 
connecting these two points. Now we have this equals zero, the 
denominator doesn't matter. In when we have eight over B equal zero 
when its to be equal zero is U, so U double derivative equal zero, that means 
that the first derivative is constant and then is that the function is 8X plus B 
that's a straight line. And A and B are found by what's called 
the boundary conditions. We know that it has to go through these 
points. So if I replace here X by a zero, I have 
to get this point and if I replace by X one, I have to get this point and in that 
way, I get A and B. So we see that for length, the function 
that basically solves the Euler-Lagrange, and that's a necessary condition, is a 
straight line. So basically, if we take this curve here, 
it will have certain length. But actually, the one that minimizes is 
not this because this is not a straight line. 
The one that minimizes is this one. So, a function to be a minimizer has to 
haul the Euler-Larange equation. Why is that important, and how do we get 
to the Euler-Lagrange equation? Let us go for a second back to regular 
functions, and how we find the minima of a regular function. 
So you might not remember exactly how we do that. 
But you do remember the result, which is going to explain next. 
The basic idea is, you have a function. And you have the function around a 
certain point. And you do a small perturbation of that 
function. And you take the derivative according to 
that perturbation. And the condition for a point to be a 
maximum or a minimum is that when you do any perturbation, the derivative is equal 
to zero. If you compute the derivative according 
to the permutation, you get this, and this has to hold for every n. 
So the derivative of the function for every end has to be equal zero which 
means that this has to be zero, and we know that. 
We know that the condition of a point to be an extreme of a function is that it's 
derivative have to be equal to zero. Now why is that so important? 
We know that but why is that so important? 
Because then I can write this differential equation. 
Xt so I can make X change in the direction of minus the derivative and 
that basically says that you start from a point and your next point, because X is 
changing in time, it just, you move a bit in the direction of the derivative of the 
function. Remember, derivative is tangent so you 
move a bit to the next spot. This is how you move. 
Your next point is this point minus a tiny step in the direction of the 
derivative. That's what this equation is telling us. 
Because if we were to discretize this equation, we get that, that. 
X. We just saw the result there. 
We had the X at T plus Delta T minus X at T. 
Divided by delta T, that's a [INAUDIBLE] equation of this. 
This expression is equal to minus fx and that's what we have here so we go slowly. 
In the direction of the derivative of X until we get to this point. 
Let's just see that again. We go one step and then we will go 
another step and another step until basically we get to the steady state. 
In the steady state, we have the XT equals zero. 
Nothing is changing anymore and, therefore, this. 
Is = to zero, which is our condition. So once again, we know that the condition 
for a point to be a minimal of a function. 
Is it has to be the derivative = to zero. So we take a point. 
I'm going to just show that to you again We take a point and we move it in the 
direction of the derivative and then slowly we are moving it in the direction 
of the derivative and we are moving it again until it doesn't move anymore. 
When it doesn't move anymore. This is equal to zero, which is the 
condition for it to be a minimum. So we arrive to a point that is in this 
case a minimum. So this concept extends to functionals, 
in the same fashion. Very simple again. 
We start from a functional, and then we're going to ask u to be in extrema, so 
we do a perturbation. So, u is here, the yellow curve. 
It's basically our original function now. We do a perturbation. 
Now, the perturbation, now, is another function. 
Because we're not talking about points. We're talking about entire functions. 
So this is the perturbation. And then u tilda is the sum of my 
function and the perturbation. And what I want is when I replace U by U 
tilde, I replace it by the perturbation and I take the derivative, according to 
the perturbation, has to be equal to zero, the same way than when I did the 
point perturbation for functions, I got zero. 
If I do a function perturbation here, I have to get zero. 
From this, after a few lines of just taking derivatives, we get the 
Euler-Lagrange equation. So that's how we get basically the 
Euler-Lagrange equation. Doing the derivative according to the 
[INAUDIBLE] that we are using. Once you have basically the 
Euler-Lagrange equation, you can do the same that we did for functions. 
You can move your function in the direction of the Euler-Lagrange equation 
until steady state, when you get to state, you have solved 
your Euler-Lagrange equation. So let's just illustrate that here. 
Look what happened. I want to put that, basically. 
I want to do that again, okay? 
So you're going to basically move until you get to the steady state. 
When you get to steady state, basically we've got zero. 
So let us recap what we just saw. In regular calculus, we basically are 
moving a point, not a curve, removing a point in the direction of minus the 
derivative and basically we get, the solution which is the minima. 
In the case of the calculus of variations, we move, we move the function 
in the direction of the Euler-Lagrange basically again the derivative for the 
perturbation. We do that all the way to stay state, 
okay? And just do that again. 
We do it all the way to steady state and at the point of steady state basically 
this became zero. And we have solved the Euler-Lagrange 
which is the condition, basically, for getting an extreme amount in this case a 
minima. We can change the signs and get the 
maxima. So we are going to see, in the next 
video, how we can take you to be an image. 
We'll do some interesting function of F. And get, for example. 
Blurring the image or get image noisy. So once again, our unknown is the image 
that basically optimizes this function. And we get the complete analogy between 
regular calculus and calculus of variations and as I say for [INAUDIBLE] 
and even more for calculus of variations, these type of techniques go way beyond 
image processing. What image processing did, and in 
particular the area of partial differential equations, is to borrow 
these techniques from continuous mathematics into this area. 
Once again, I'm going to show you in the next video an example of a function that 
is very useful for image processing. I see you in the next video. 
Thank you very much.